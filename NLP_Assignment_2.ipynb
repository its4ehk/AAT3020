{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/its4ehk/AAT3020/blob/main/NLP_Assignment_2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0654ab8c",
      "metadata": {
        "id": "0654ab8c"
      },
      "source": [
        "# Assignment 2\n",
        "\n",
        "- In this assignment, you will train a model for sentiment analysis\n",
        "    - Sentiment analysis is to predict whether the given text's sentiment is positive or negative\n",
        "    - The input is a sequence of tokens and the output is a result of sigmoid function\n",
        "- You have to submit a report (in pdf) and your code (in .py)\n",
        "    - In your report, you have to briefly explain about your code\n",
        "        - Code explanation can be very simple\n",
        "    - Also, add explanation on these problems\n",
        "        - Problem 6: Analyze the Prediction of Model\n",
        "    - You have to submit your code in .py file\n",
        "        - Copy and paste your completed code to ``NLP_Assignment_2.py`` file\n",
        "- The main goal of this assignment is to implement a pipeline to train a neural network\n",
        "    - Problem 1: Building a Dataset class (6 pts)\n",
        "    - Problem 2: Build a Str2Idx2Str Converter (12 pts)\n",
        "        - Complete the function\n",
        "    - Problem 3: Implement a collate function (7 pts)\n",
        "    - Problem 4: Implement a Binary Cross Entropy Loss (5 pts)\n",
        "    - Problem 5: Complete Training Loop (12 pts)\n",
        "        - Training with a single batch\n",
        "        - Validate the model\n",
        "    - Problem 6: Analyze the Prediction of Model (18 pts)\n",
        "        - Write it in your report"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "id": "c31717fb",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c31717fb",
        "outputId": "937686d4-f713-42f7-a2e1-8474583070aa",
        "collapsed": true
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2025-04-15 11:24:42--  https://raw.githubusercontent.com/jdasam/aat3020/2025/NLP_Assignment_2.py\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 16729 (16K) [text/plain]\n",
            "Saving to: ‘NLP_Assignment_2.py.2’\n",
            "\n",
            "\rNLP_Assignment_2.py   0%[                    ]       0  --.-KB/s               \rNLP_Assignment_2.py 100%[===================>]  16.34K  --.-KB/s    in 0s      \n",
            "\n",
            "2025-04-15 11:24:42 (71.1 MB/s) - ‘NLP_Assignment_2.py.2’ saved [16729/16729]\n",
            "\n",
            "--2025-04-15 11:24:42--  https://raw.githubusercontent.com/jdasam/aat3020/2025/NLP_assignment_2_pre_defined.py\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.110.133, 185.199.108.133, 185.199.111.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.110.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 404 Not Found\n",
            "2025-04-15 11:24:42 ERROR 404: Not Found.\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Download .py file. You have to copy and paste the completed function to this py file and submit it.\n",
        "!wget https://raw.githubusercontent.com/jdasam/aat3020/2025/NLP_Assignment_2.py\n",
        "!wget https://raw.githubusercontent.com/jdasam/aat3020/2025/NLP_assignment_2_pre_defined.py"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "737b088c",
      "metadata": {
        "id": "737b088c"
      },
      "source": [
        "## Preparation: Download dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "5531281c",
      "metadata": {
        "id": "5531281c",
        "collapsed": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4863f6c3-f7f0-4146-b922-54c642d56ece"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2025-04-15 10:26:29--  https://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\n",
            "Resolving ai.stanford.edu (ai.stanford.edu)... 171.64.68.10\n",
            "Connecting to ai.stanford.edu (ai.stanford.edu)|171.64.68.10|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 84125825 (80M) [application/x-gzip]\n",
            "Saving to: ‘aclImdb_v1.tar.gz.1’\n",
            "\n",
            "aclImdb_v1.tar.gz.1 100%[===================>]  80.23M  17.9MB/s    in 6.9s    \n",
            "\n",
            "2025-04-15 10:26:36 (11.7 MB/s) - ‘aclImdb_v1.tar.gz.1’ saved [84125825/84125825]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!wget https://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz # download dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "2f68924c",
      "metadata": {
        "id": "2f68924c"
      },
      "outputs": [],
      "source": [
        "!tar -xzf aclImdb_v1.tar.gz # unzip the file"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "id": "937695d9",
      "metadata": {
        "id": "937695d9"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "import random\n",
        "from pathlib import Path\n",
        "from typing import List, Tuple, Union\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8b26a09b",
      "metadata": {
        "id": "8b26a09b"
      },
      "source": [
        "### Make Datasplit\n",
        "- In typical machine learning tasks, one has to split training set and validation set\n",
        "    - Training set is to train the model's parameter\n",
        "    - Validation set is to check how model works for unseen dataset\n",
        "        - Validation set is used to optimize model's hyperparameter"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "id": "ab60c3b3",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ab60c3b3",
        "outputId": "75354bc8-75b1-41d8-b0f2-a040e2ae2b57"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of training data: 20000, validation data: 5000, test data: 25002\n"
          ]
        }
      ],
      "source": [
        "'''\n",
        "You don't have to change this cell\n",
        "'''\n",
        "\n",
        "\n",
        "train_path = Path('aclImdb/train')\n",
        "test_path = Path('aclImdb/test')\n",
        "\n",
        "def get_train_txt_paths_in_split(dir_path:str='aclImdb/train', seed:int=0):\n",
        "  dir_path = Path(dir_path)\n",
        "  train_set, valid_set = [], []\n",
        "  random.seed(seed) # manually seed random so that you can get the same random result whenever you run the code for reproducibility\n",
        "  for typ in ('pos', 'neg'):\n",
        "    paths_of_typ = list( (dir_path / typ).glob('*.txt'))\n",
        "    num_examples = len(paths_of_typ)\n",
        "    num_train_sample = num_examples * 4 // 5\n",
        "\n",
        "    paths_of_typ = sorted(paths_of_typ)\n",
        "    random.shuffle(paths_of_typ) # shuffle the dataset\n",
        "    train_set += paths_of_typ[:num_train_sample] # assign first num_train_sample samples for train set\n",
        "    valid_set += paths_of_typ[num_train_sample:] # assign the remaining samples for validation set\n",
        "\n",
        "  random.shuffle(train_set)\n",
        "  random.shuffle(valid_set)\n",
        "\n",
        "  return train_set, valid_set\n",
        "\n",
        "\n",
        "train_pths, valid_pths = get_train_txt_paths_in_split(train_path)\n",
        "test_pths = list(test_path.rglob(\"*.txt\"))\n",
        "\n",
        "print(f\"Number of training data: {len(train_pths)}, validation data: {len(valid_pths)}, test data: {len(test_pths)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "id": "42ad6145",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "42ad6145",
        "outputId": "bbf1ca2e-aee8-4f40-ef8c-4552c78e2c7a"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[PosixPath('aclImdb/train/neg/6600_1.txt'),\n",
              " PosixPath('aclImdb/train/neg/9945_1.txt'),\n",
              " PosixPath('aclImdb/train/pos/7684_7.txt'),\n",
              " PosixPath('aclImdb/train/pos/2614_9.txt'),\n",
              " PosixPath('aclImdb/train/pos/3662_9.txt'),\n",
              " PosixPath('aclImdb/train/pos/2431_8.txt'),\n",
              " PosixPath('aclImdb/train/pos/8332_9.txt'),\n",
              " PosixPath('aclImdb/train/neg/992_1.txt'),\n",
              " PosixPath('aclImdb/train/pos/8901_8.txt'),\n",
              " PosixPath('aclImdb/train/pos/1301_10.txt')]"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ],
      "source": [
        "'''\n",
        " Print the first 10 paths in train_pths\n",
        "'''\n",
        "train_pths[:10]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "acb1ca09",
      "metadata": {
        "id": "acb1ca09"
      },
      "source": [
        "#### Make Vocabulary\n",
        "- This cell makes a vocabulary from the training set\n",
        "  - using ``basic_english`` tokenizer\n",
        "  - using ``Counter`` to count the number of tokens\n",
        "  - using ``min_count`` to remove tokens that appear less than ``min_count`` times"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "id": "nuTYM6w3vrnk",
      "metadata": {
        "id": "nuTYM6w3vrnk"
      },
      "outputs": [],
      "source": [
        "# Tokenizer from torchtext\n",
        "\n",
        "class Tokenizer:\n",
        "  _patterns = [r'\\'', r'\\\"', r'\\.', r'<br \\/>', r',', r'\\(', r'\\)', r'\\!', r'\\?', r'\\;', r'\\:', r'\\s+']\n",
        "\n",
        "  _replacements = [' \\'  ', '', ' . ', ' ', ' , ', ' ( ', ' ) ', ' ! ', ' ? ',     ' ',    ' ',   ' ']\n",
        "\n",
        "  _patterns_dict = list((re.compile(p), r) for p, r in zip(_patterns, _replacements))\n",
        "\n",
        "\n",
        "  def __call__(self, line):\n",
        "      r\"\"\"\n",
        "      Basic normalization for a line of text.\n",
        "      Normalization includes\n",
        "      - lowercasing\n",
        "      - complete some basic text normalization for English words as follows:\n",
        "          add spaces before and after '\\''\n",
        "          remove '\\\"',\n",
        "          add spaces before and after '.'\n",
        "          replace '<br \\/>'with single space\n",
        "          add spaces before and after ','\n",
        "          add spaces before and after '('\n",
        "          add spaces before and after ')'\n",
        "          add spaces before and after '!'\n",
        "          add spaces before and after '?'\n",
        "          replace ';' with single space\n",
        "          replace ':' with single space\n",
        "          replace multiple spaces with single space\n",
        "\n",
        "      Returns a list of tokens after splitting on whitespace.\n",
        "      \"\"\"\n",
        "\n",
        "      line = line.lower()\n",
        "      for pattern_re, replaced_str in self._patterns_dict:\n",
        "          line = pattern_re.sub(replaced_str, line)\n",
        "      return line.split()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "id": "1426d9c3",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1426d9c3",
        "outputId": "74941f07-1418-4473-87fe-2a9ab4a3cb33"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of tokens in the entire vocabulary: 90072\n",
            "Number of tokens in the vocabulary with min_count = 5: 27030\n",
            "First 10 tokens in the vocabulary: ['!', '#', '#1', '#2', '#3', '#4', '#5', '#9', '$', '$$']\n",
            "Last 10 tokens in the vocabulary: ['\\x96', '\\x97', '¡¨', '£1', '£3', '·', '½', 'à', '–', '\\uf0b7']\n",
            "Middle 10 tokens in the vocabulary: ['knee-jerk', 'knees', 'knef', 'knew', 'knickers', 'knife', 'knifes', 'knight', 'knightley', 'knightly']\n"
          ]
        }
      ],
      "source": [
        "from collections import Counter\n",
        "\n",
        "def make_vocab_from_txt_fns(txt_fns_list:List[Path], tokenizer):\n",
        "  '''\n",
        "  This function takes a list of txt file paths and returns a list of all the words in the txt files\n",
        "  '''\n",
        "  vocab = Counter()\n",
        "  for txt_fn in txt_fns_list:\n",
        "    with open(txt_fn, 'r') as f:\n",
        "      for line in f:\n",
        "        vocab.update(tokenizer(line))\n",
        "  return vocab\n",
        "\n",
        "tokenizer = Tokenizer()\n",
        "entire_vocab = make_vocab_from_txt_fns(train_pths, tokenizer)\n",
        "min_count = 5\n",
        "vocab = sorted([token for token, count in entire_vocab.items() if count >= min_count])\n",
        "\n",
        "print(f\"Number of tokens in the entire vocabulary: {len(entire_vocab)}\")\n",
        "print(f\"Number of tokens in the vocabulary with min_count = {min_count}: {len(vocab)}\")\n",
        "print(f\"First 10 tokens in the vocabulary: {vocab[:10]}\")\n",
        "print(f\"Last 10 tokens in the vocabulary: {vocab[-10:]}\")\n",
        "print(f\"Middle 10 tokens in the vocabulary: {vocab[len(vocab)//2-5:len(vocab)//2+5]}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "69b8f868",
      "metadata": {
        "id": "69b8f868"
      },
      "source": [
        "## Problem 1: Complete the dataset class\n",
        "- Complete the given class ``IMDbData``\n",
        "    - ``IMDbData`` has a list of txt paths. Each txt corresponds to a single data sample.\n",
        "        - **The label, whether the given sentence is positive or negative, is recorded in the name of directory path of the file**\n",
        "        - You can convert ``Path`` instance to ``str`` by ``str(a_path)``\n",
        "    - Complete two special methods ``__len__`` and ``__getitem__``\n",
        "        - ``__len__`` returns the length of the dataset, which is number of total data samples in the dataset\n",
        "        - ``__getitem__`` takes an index and returns the corresponding data sample for a given index\n",
        "        - To read txt file, you can use a pre-defined ``read_txt`` function\n",
        "            - ``read_txt`` gets an txt file path as an input and returns a content of the txt file in a string\n",
        "        - To get a list of token, use ``self.tokenizer``"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "id": "6ed1a575",
      "metadata": {
        "id": "6ed1a575",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a6c495b6-9734-4f6b-a044-6632a571c785"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "__len__ result for trainset:  20000\n",
            "__getitem__ result:  (['after', 'watching', 'about', 'half', 'of', 'this', 'movie', 'i', 'noticed', 'something', 'peculiar', '.', '.', '.', 'i', 'found', 'myself', 'constantly', 'switching', 'through', 'tv-channels', 'to', 'see', 'what', 'else', 'is', 'on', '-', 'not', 'exactly', 'a', 'good', 'movie', 'trait', '.', 'this', 'movie', 'is', 'listed', 'as', 'being', 'in', 'a', 'number', 'of', 'genres', ',', 'and', 'i', 'must', 'say', 'it', 'mostly', 'failed', 'misserably', 'in', 'every', 'one', 'of', 'them', '.', '80%', 'through', 'the', 'movie', 'i', 'switched', 'over', 'to', 'watch', 'an', 'old', 'rerun', 'instead', '.', 'bottom', 'line', '-', 'the', 'whole', 'movie', 'felt', 'as', 'if', 'the', 'ones', 'making', 'it', 'didn', \"'\", 't', 'exactly', 'know', 'what', 'to', 'make', 'and', 'ended', 'up', 'in', 'a', 'concoction', 'with', 'no', 'discernable', 'taste', '.'], 0)\n"
          ]
        }
      ],
      "source": [
        "def read_txt(txt_path):\n",
        "  with open(txt_path, 'r') as f:\n",
        "    txt_string = f.readline()\n",
        "  return txt_string\n",
        "\n",
        "class IMDbData:\n",
        "  def __init__(self, path_list):\n",
        "    self.paths = path_list\n",
        "    self.tokenizer = Tokenizer()\n",
        "\n",
        "  def __len__(self) -> int:\n",
        "    return len(self.paths)\n",
        "\n",
        "  def __getitem__(self, idx:int) -> Tuple[List[str], int]:\n",
        "    path = self.paths[idx]\n",
        "    text = read_txt(path)\n",
        "    tokens = self.tokenizer(text)\n",
        "    label = 0 if 'neg' in str(path) else 1\n",
        "    return tokens, label\n",
        "\n",
        "trainset = IMDbData(train_pths)\n",
        "validset = IMDbData(valid_pths)\n",
        "short_validset = IMDbData(valid_pths[:100])\n",
        "testset = IMDbData(test_pths)\n",
        "\n",
        "print('__len__ result for trainset: ', len(trainset))\n",
        "print('__getitem__ result: ', trainset[1])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "id": "345935b6",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "345935b6",
        "outputId": "9814ac05-79ab-4ff4-cea2-ab1a932785e4",
        "collapsed": true
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['i',\n",
              " \"'\",\n",
              " 'm',\n",
              " 'working',\n",
              " 'on',\n",
              " 'assignment',\n",
              " '2',\n",
              " 'for',\n",
              " 'my',\n",
              " 'nlp',\n",
              " 'class',\n",
              " '!']"
            ]
          },
          "metadata": {},
          "execution_count": 40
        }
      ],
      "source": [
        "'''\n",
        "Examples of how tokenizer works\n",
        "'''\n",
        "trainset.tokenizer(\"I'm working on assignment 2 for my NLP class!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "id": "d73b5f1a",
      "metadata": {
        "id": "d73b5f1a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "34e3e114-49c0-4f17-f3cd-11c7a7ea700f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Passed all the test cases!\n"
          ]
        }
      ],
      "source": [
        "'''\n",
        "Test your IMDbData class\n",
        "'''\n",
        "trainset = IMDbData(train_pths)\n",
        "assert len(trainset) == 20000 and len(validset) ==5000 and len(short_validset)==100\n",
        "assert len(trainset[0]) == 2\n",
        "assert trainset[154][0][10:15] == ['ends', 'right', 'after', 'this', 'little'], \"Error in the trainset __getitem__ output\"\n",
        "assert trainset[594][1] == 0 and trainset[523][1] == 1 and trainset[1523][1] == 0, \"Error in the trainset __getitem__ output\"\n",
        "\n",
        "print(\"Passed all the test cases!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0a3e235a",
      "metadata": {
        "id": "0a3e235a"
      },
      "source": [
        "## Problem 2: Complete String to idx Converter\n",
        "- Complete a class for converting a list of string to a list of integer\n",
        "    - use the variable ``vocab``, which is a list of strings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "id": "1dac770a",
      "metadata": {
        "id": "1dac770a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "842b8af8-f095-4811-e680-00f037a80386"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input sentence: ['yowza', '!', 'if', 'anyone', 'who', 'loves', 'laurel', 'and', 'hardy', 'can', 'watch', 'this', 'movie', 'and', 'feel', 'good', 'about', 'it', ',', 'you']\n",
            "Converted sentence: [27030, 0, 11987, 1487, 26426, 14417, 13830, 1310, 11072, 3792, 26166, 24238, 15911, 1310, 9041, 10407, 551, 12881, 76, 26889]\n",
            "Re-converted sentence: ['UNKNOWN', '!', 'if', 'anyone', 'who', 'loves', 'laurel', 'and', 'hardy', 'can', 'watch', 'this', 'movie', 'and', 'feel', 'good', 'about', 'it', ',', 'you']\n",
            "Result for a list of sentences/ input_list: [['after', 'watching', 'about', 'half', 'of'], ['reading', 'all', 'of', 'the', 'comments'], ['why', 'has', 'this', 'not', 'been'], ['this', 'is', 'a', 'really', 'strange']], output_list: [[902, 26173, 551, 10912, 16765], [19394, 1092, 16765, 24153, 5020], [26447, 11120, 24238, 16550, 2483], [24238, 12847, 493, 19419, 23088]]\n"
          ]
        }
      ],
      "source": [
        "class Str2Idx2Str:\n",
        "  def __init__(self, vocab: List[str]):\n",
        "    # Create a list mapping index to string (token)\n",
        "    self.idx2str = vocab.copy()\n",
        "\n",
        "    # Create a dictionary mapping string (token) to index\n",
        "    self.str2idx = {word: idx for idx, word in enumerate(self.idx2str)}\n",
        "\n",
        "    # Handel out of vocabulary words by addying an \"UNKNOWN\" token\n",
        "    self.unknown_idx = len(self.str2idx)\n",
        "    self.idx2str.append(\"UNKNOWN\")\n",
        "\n",
        "  def __call__(self, alist:Union[List[str], List[int], str, int, List[List]]) -> Union[List[int], List[str], int, str, List[List]]:\n",
        "    # If input is a single string → return index\n",
        "    if isinstance(alist, str):\n",
        "      return self.str2idx.get(alist, self.unknown_idx)\n",
        "\n",
        "    # If input is a single index → return string\n",
        "    elif isinstance(alist, int):\n",
        "      return self.idx2str[alist] if alist < len(self.idx2str) else \"UNKNOWN\"\n",
        "\n",
        "    # If input is a list\n",
        "    elif isinstance(alist, list):\n",
        "      # If list of lists → recursive processing\n",
        "      if isinstance(alist[0], list):\n",
        "        return [self(sublist) for sublist in alist]\n",
        "\n",
        "      # If list of strings → convert each to index\n",
        "      elif isinstance(alist[0], str):\n",
        "        return [self.str2idx.get(tok, self.unknown_idx) for tok in alist]\n",
        "\n",
        "      # If list of indices → convert each to string\n",
        "      elif isinstance(alist[0], int):\n",
        "        return [self.idx2str[i] if i < len(self.idx2str) else \"UNKNOWN\" for i in alist]\n",
        "\n",
        "    # If input type is unexpected, return as is\n",
        "    return alist\n",
        "\n",
        "# Test the code\n",
        "converter = Str2Idx2Str(vocab)\n",
        "input_sentence = trainset[0][0][:20] #0th sample, text (instead of label), first 20 words\n",
        "print(f\"Input sentence: {input_sentence}\")\n",
        "print(f\"Converted sentence: {converter(input_sentence)}\")\n",
        "print(f\"Re-converted sentence: {converter(converter(input_sentence))}\")\n",
        "print(f\"Result for a list of sentences/ input_list: {[trainset[i][0][:5]for i in range(1,5)]}, output_list: {converter([trainset[i][0][:5]for i in range(1,5)])}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "id": "4edac0e1",
      "metadata": {
        "id": "4edac0e1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dd22f62c-8f84-465b-b65b-fbde404bbb54"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Passed all the test cases!\n"
          ]
        }
      ],
      "source": [
        "'''\n",
        "Test your code by running this cell.\n",
        "\n",
        "Don't change the test cases\n",
        "'''\n",
        "\n",
        "list_of_string = ['if', 'anyone', 'who', 'loves', 'laurel', 'and', 'hardy', 'can', 'watch', 'this', 'movie', 'and', 'feel', 'good', 'about', 'it', ',', 'you']\n",
        "list_of_intger = [11987, 1487, 26426, 14417, 13830, 1310, 11072, 3792, 26166, 24238, 15911, 1310, 9041, 10407, 551, 12881, 76, 26889]\n",
        "\n",
        "assert converter(list_of_string) == list_of_intger, \\\n",
        "    f\"The output of converting list_of_string has to be same with list_of_intger. Your current output is {converter(list_of_string)}\"\n",
        "assert converter(list_of_intger) == list_of_string, \\\n",
        "    f\"The output of converting list_of_intger has to be same with list_of_string. Your current output is {converter(list_of_intger)}\"\n",
        "\n",
        "\n",
        "list_of_string_list = [['after', 'watching', 'about', 'half', 'of'], ['reading', 'all', 'of', 'the', 'comments'], ['why', 'has', 'this', 'not', 'been'], ['this', 'is', 'a', 'really', 'strange']]\n",
        "list_of_integer_list = [[902, 26173, 551, 10912, 16765], [19394, 1092, 16765, 24153, 5020], [26447, 11120, 24238, 16550, 2483], [24238, 12847, 493, 19419, 23088]]\n",
        "\n",
        "assert converter(list_of_string_list) == list_of_integer_list, \\\n",
        "    f\"The output of converting list_of_string_list has to be same with list_of_integer_list. Your current output is {converter(list_of_string_list)}\"\n",
        "assert converter(list_of_integer_list) == list_of_string_list, \\\n",
        "    f\"The output of converting list_of_integer_list has to be same with list_of_string_list. Your current output is {converter(list_of_integer_list)}\"\n",
        "\n",
        "print(\"Passed all the test cases!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0274efa4",
      "metadata": {
        "id": "0274efa4"
      },
      "source": [
        "## Problem 3: Complete Collate Function\n",
        "- Every data sample in ``IMDbData`` has different length\n",
        "    - Therefore, you have to handle various input length to group the multiple sequnece samples as a tensor\n",
        "- You have to implement ``pack_collate`` which takes a raw batch from the dataset and groups it into a ``PackedSequence``\n",
        "    - You don't need to know about ``PackedSequence`` now. It helps to implement an efficient computation for sequence with different lengths\n",
        "- Implement two variables, following the description in the function\n",
        "    - ``txts_in_idxs``, ``labels``\n",
        "    - use ``self.converter``"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "id": "85545f25",
      "metadata": {
        "id": "85545f25",
        "collapsed": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0610bc0f-b0ea-4377-e2a0-06d0fc83e3cd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "A batch looks like this:  (PackedSequence(data=tensor([18325, 24238, 11556,  ...,  9166, 16023,    94]), batch_sizes=tensor([32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32,\n",
            "        32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32,\n",
            "        32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32,\n",
            "        32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 31, 31, 31, 31, 31,\n",
            "        31, 31, 31, 31, 31, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30,\n",
            "        30, 30, 30, 30, 29, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28,\n",
            "        28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28,\n",
            "        28, 28, 28, 28, 27, 27, 27, 27, 27, 27, 27, 27, 27, 26, 26, 26, 26, 26,\n",
            "        26, 26, 26, 25, 25, 25, 25, 25, 25, 24, 24, 24, 24, 24, 24, 24, 24, 24,\n",
            "        23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 22,\n",
            "        21, 21, 20, 20, 20, 20, 20, 20, 20, 20, 20, 19, 19, 19, 19, 19, 19, 19,\n",
            "        19, 19, 19, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18,\n",
            "        18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18,\n",
            "        18, 18, 18, 18, 18, 18, 18, 18, 18, 17, 17, 17, 17, 16, 16, 16, 16, 16,\n",
            "        16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16,\n",
            "        16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16,\n",
            "        16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16,\n",
            "        16, 16, 16, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15,\n",
            "        15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 14,\n",
            "        14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14,\n",
            "        14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14,\n",
            "        14, 14, 14, 14, 14, 14, 14, 14, 14, 13, 13, 13, 13, 13, 13, 13, 13, 13,\n",
            "        13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13,\n",
            "        13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13,\n",
            "        13, 13, 13, 12, 12, 12, 10, 10, 10,  9,  9,  9,  9,  9,  9,  9,  9,  9,\n",
            "         9,  9,  9,  9,  9,  9,  9,  9,  9,  9,  9,  9,  9,  8,  8,  8,  8,  8,\n",
            "         8,  8,  8,  8,  8,  8,  8,  8,  8,  8,  8,  8,  8,  8,  8,  8,  7,  7,\n",
            "         7,  7,  6,  6,  6,  6,  6,  5,  5,  5,  5,  5,  5,  5,  5,  5,  5,  5,\n",
            "         5,  5,  5,  5,  5,  5,  5,  5,  5,  5,  5,  5,  5,  5,  5,  5,  5,  5,\n",
            "         5,  5,  5,  5,  5,  5,  5,  5,  5,  5,  5,  5,  5,  5,  5,  5,  5,  5,\n",
            "         5,  4,  4,  4,  4,  4,  4,  4,  4,  4,  4,  4,  4,  4,  4,  4,  4,  4,\n",
            "         4,  4,  4,  4,  4,  4,  4,  4,  4,  4,  4,  4,  4,  4,  4,  4,  4,  4,\n",
            "         4,  4,  4,  4,  4,  4,  4,  4,  4,  4,  4,  4,  4,  4,  4,  4,  4,  4,\n",
            "         4,  4,  4,  4,  4,  4,  4,  4,  4,  4,  4,  4,  4,  4,  4,  4,  4,  4,\n",
            "         4,  4,  4,  3,  3,  3,  3,  3,  3,  3,  3,  3,  3,  3,  3,  3,  3,  3,\n",
            "         3,  3,  3,  3,  3,  3,  3,  3,  3,  2,  2,  2,  2,  2,  2,  2,  2,  2,\n",
            "         2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,\n",
            "         2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,\n",
            "         2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,\n",
            "         2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,\n",
            "         2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,\n",
            "         2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,\n",
            "         2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,\n",
            "         2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,\n",
            "         2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,\n",
            "         2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,\n",
            "         2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,\n",
            "         2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,\n",
            "         2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,\n",
            "         2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  1,  1,  1,  1,  1,  1,  1,  1,\n",
            "         1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,\n",
            "         1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,\n",
            "         1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,\n",
            "         1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,\n",
            "         1,  1,  1,  1]), sorted_indices=tensor([ 9, 20,  4, 25, 19,  6, 10,  0, 13, 26,  5, 31, 23, 14, 24, 27, 28,  3,\n",
            "        12,  1, 16, 22, 21,  8, 30, 15, 18, 11,  7, 17, 29,  2]), unsorted_indices=tensor([ 7, 19, 31, 17,  2, 10,  5, 28, 23,  0,  6, 27, 18,  8, 13, 25, 20, 29,\n",
            "        26,  4,  1, 22, 21, 12, 14,  3,  9, 15, 16, 30, 24, 11])), tensor([0., 1., 1., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 1., 0., 1., 0.,\n",
            "        0., 1., 1., 1., 1., 0., 0., 0., 0., 0., 1., 0., 0., 0.]))\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.nn.utils.rnn import PackedSequence, pad_packed_sequence, pack_sequence\n",
        "\n",
        "class PackCollateWithConverter:\n",
        "  def __init__(self, converter: Str2Idx2Str):\n",
        "    self.converter = converter\n",
        "\n",
        "  def __call__(self, batch: List[Tuple[List[str], int]]):\n",
        "    word_sentences_in_idxs = [torch.LongTensor(self.converter(tokens)) for tokens, _ in batch]\n",
        "    label_tensor = torch.FloatTensor([float(label) for _, label in batch])\n",
        "\n",
        "    '''\n",
        "    Leave the code below as it is\n",
        "    '''\n",
        "    assert isinstance(word_sentences_in_idxs, list), f\"txts_in_idxs has to be a list, not {type(word_sentences_in_idxs)}\"\n",
        "    assert isinstance(word_sentences_in_idxs[0], torch.LongTensor), f\"An elmenet of txts_in_idxs has to be a torch.LongTensor, not {type(word_sentences_in_idxs[0])}\"\n",
        "    assert isinstance(label_tensor, torch.FloatTensor), f\"labels has to be a torch.FloatTensor, not {type(label_tensor)}\"\n",
        "    assert label_tensor[-1] == batch[-1][1], \"i-th element of labels has to be \"\n",
        "\n",
        "    packed_sequence = pack_sequence(word_sentences_in_idxs, enforce_sorted=False)\n",
        "\n",
        "    return packed_sequence, label_tensor\n",
        "\n",
        "pack_collate = PackCollateWithConverter(converter)\n",
        "# Test the code\n",
        "train_loader = DataLoader(trainset, batch_size=32, collate_fn=pack_collate, shuffle=True)\n",
        "batch = next(iter(train_loader))\n",
        "\n",
        "print('A batch looks like this: ', batch)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fc96e0d1",
      "metadata": {
        "id": "fc96e0d1"
      },
      "source": [
        "### Preparation: Define Model\n",
        "- You don't have to change this code, or try to understand how this GRU model works at the current stage"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "id": "fa84a216",
      "metadata": {
        "id": "fa84a216",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ba142e0f-20b9-45b6-be54-2e9a5959b28a"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([0.5325, 0.5204, 0.5188, 0.5238, 0.5200, 0.5223, 0.5276, 0.5283, 0.5262,\n",
              "        0.5149, 0.5027, 0.5264, 0.5157, 0.5097, 0.5213, 0.5282, 0.5264, 0.5262,\n",
              "        0.5287, 0.5130, 0.5261, 0.5191, 0.5204, 0.5397, 0.5199, 0.5018, 0.5244,\n",
              "        0.5302, 0.5145, 0.5169, 0.5207, 0.5208], grad_fn=<SigmoidBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 44
        }
      ],
      "source": [
        "class SentimentModel(nn.Module):\n",
        "  def __init__(self,vocab_size, hidden_size=128, num_layers=3):\n",
        "    super().__init__()\n",
        "    self.word_embedding = nn.Embedding(vocab_size, hidden_size)\n",
        "    self.gru = nn.GRU(hidden_size, hidden_size, num_layers=num_layers, bidirectional=True, dropout=0.3, batch_first=True)\n",
        "    self.num_layers = num_layers\n",
        "    self.hidden_size = hidden_size\n",
        "    self.final_layer = nn.Linear(hidden_size*2, 1)\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = PackedSequence(self.word_embedding(x.data), batch_sizes=x.batch_sizes, sorted_indices=x.sorted_indices, unsorted_indices=x.unsorted_indices)\n",
        "    x, hidden = self.gru(x)\n",
        "    pad_x, lens = pad_packed_sequence(x, batch_first=True)\n",
        "\n",
        "    max_x = torch.stack([torch.max(pad_x[i, :lens[i]], dim=0)[0] for i in range(len(lens))], dim=0)\n",
        "\n",
        "    # max_x = torch.max(pad_x, dim=0)[0]\n",
        "    pred_logit = self.final_layer(max_x)[:,0]\n",
        "    return torch.sigmoid(pred_logit)\n",
        "\n",
        "# Test the model\n",
        "model = SentimentModel(len(converter.idx2str))\n",
        "batch = next(iter(train_loader))\n",
        "x, y = batch\n",
        "out = model(x)\n",
        "out"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1e8e45ef",
      "metadata": {
        "id": "1e8e45ef"
      },
      "source": [
        "## Problem 4: Implement Binary Cross Entropy Loss\n",
        "- Without using ``torch.nn.BCELoss``\n",
        "    - You can implement it with ``torch.log`` and ``torch.mean`` or ``atensor.mean()``"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "id": "90e39022",
      "metadata": {
        "id": "90e39022"
      },
      "outputs": [],
      "source": [
        "def get_binary_cross_entropy_loss(pred:torch.FloatTensor, target:torch.FloatTensor, eps=1e-8) -> torch.FloatTensor:\n",
        "  loss = -(target * torch.log(pred+eps)+(1-target)*torch.log(1-pred+eps))\n",
        "  return loss.mean()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "id": "e99634eb",
      "metadata": {
        "id": "e99634eb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "47e33c27-366a-47b2-d441-0e004e4b9455"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "BCE Loss by torch.nn.BCELoss is 0.09634757786989212 and your BCE loss is 0.09634757786989212\n"
          ]
        }
      ],
      "source": [
        "'''\n",
        "Test your BCE loss function\n",
        "Don't change the test cases\n",
        "'''\n",
        "\n",
        "test_pred_case = torch.Tensor([9.9894e-01, 2.2645e-03, 1.8131e-01, 8.0153e-03, 9.9972e-01, 1.0378e-03,\n",
        "        9.9949e-01, 9.9967e-01, 6.4150e-03, 9.9912e-01, 9.9896e-01, 1.4350e-01,\n",
        "        9.9896e-01, 2.1979e-02, 9.9976e-01, 4.5389e-03, 9.9906e-01, 1.0633e-02,\n",
        "        9.9749e-01, 5.5501e-04, 7.0052e-04, 2.9509e-04, 3.2752e-04, 9.9940e-01,\n",
        "        4.5912e-04, 9.9969e-01, 6.0225e-03, 9.9974e-01, 9.9907e-01, 9.9942e-01,\n",
        "        4.0911e-01, 2.8850e-01])\n",
        "test_target_case = torch.Tensor([1., 0., 0., 0., 1., 0., 1., 1., 0., 1., 1., 1., 1., 0., 1., 0., 1., 0.,\n",
        "        1., 0., 0., 0., 0., 1., 0., 1., 0., 1., 1., 1., 0., 0.])\n",
        "\n",
        "your_result = get_binary_cross_entropy_loss(test_pred_case, test_target_case)\n",
        "\n",
        "'''\n",
        "The value can be little different because of epsilon value used for torch.log\n",
        "'''\n",
        "print(f\"BCE Loss by torch.nn.BCELoss is {torch.nn.BCELoss()(test_pred_case, test_target_case)} and your BCE loss is {your_result}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2497890e",
      "metadata": {
        "id": "2497890e"
      },
      "source": [
        "## Problem 5: Complete Training Loop\n",
        "- In this problem, you have to implement a Trainer class\n",
        "    - It contains everything you need to train a neural network model\n",
        "    - model, optimizer, loss function, train loader, validation loader, and device (cuda or cpu)\n",
        "- IMPORTANT\n",
        "    - Select proper ``batch_size`` for ``train_loader``, ``validation_loader``, ``test_loader``\n",
        "\n",
        "- Complete ``self._get_accuracy()``\n",
        "  - If the prediction is larger than 0.5, you can regard it as positive sentiment\n",
        "\n",
        "- Complete ``self._get_loss_and_acc_from_single_batch()``\n",
        "    - using ``self._get_accuracy()``\n",
        "\n",
        "- Complete ``_train_by_single_batch``\n",
        "    - using ``self._get_loss_and_acc_from_single_batch()``\n",
        "    - You can test it on the cell below\n",
        "\n",
        "- Complete ``validate``\n",
        "    - Implement it with **preventing gradient calculation** to reduce memory usage\n",
        "        - Use ``with torch.no_grad():`` or ``with torch.inference_mode():``\n",
        "    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "id": "7246fae3",
      "metadata": {
        "id": "7246fae3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 339
        },
        "outputId": "21b5430b-64b7-45cb-a16d-244ac07ee0fb"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "Found no NVIDIA driver on your system. Please check that you have an NVIDIA GPU and installed a driver from http://www.nvidia.com/Download/index.aspx",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-53-558952f600d2>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    178\u001b[0m \u001b[0mtest_loader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDataLoader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtestset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m128\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcollate_fn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpack_collate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    179\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 180\u001b[0;31m \u001b[0mtrainer\u001b[0m \u001b[0;34m=\u001b[0m  \u001b[0mTrainer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mget_binary_cross_entropy_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalid_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'cuda'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-53-558952f600d2>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, model, optimizer, loss_fn, train_loader, valid_loader, device)\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalid_loader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalid_loader\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbest_valid_accuracy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36mto\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1341\u001b[0m                     \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1342\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1343\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconvert\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1344\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1345\u001b[0m     def register_full_backward_pre_hook(\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    901\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrecurse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    902\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 903\u001b[0;31m                 \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    904\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    905\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor_applied\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    928\u001b[0m             \u001b[0;31m# `with torch.no_grad():`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    929\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 930\u001b[0;31m                 \u001b[0mparam_applied\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    931\u001b[0m             \u001b[0mp_should_use_set_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparam_applied\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    932\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36mconvert\u001b[0;34m(t)\u001b[0m\n\u001b[1;32m   1327\u001b[0m                         \u001b[0mmemory_format\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconvert_to_format\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1328\u001b[0m                     )\n\u001b[0;32m-> 1329\u001b[0;31m                 return t.to(\n\u001b[0m\u001b[1;32m   1330\u001b[0m                     \u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1331\u001b[0m                     \u001b[0mdtype\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_floating_point\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_complex\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/cuda/__init__.py\u001b[0m in \u001b[0;36m_lazy_init\u001b[0;34m()\u001b[0m\n\u001b[1;32m    317\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;34m\"CUDA_MODULE_LOADING\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menviron\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    318\u001b[0m             \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menviron\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"CUDA_MODULE_LOADING\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"LAZY\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 319\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_cuda_init\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    320\u001b[0m         \u001b[0;31m# Some of the queued calls may reentrantly call _lazy_init();\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    321\u001b[0m         \u001b[0;31m# we need to just return without initializing in that case.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: Found no NVIDIA driver on your system. Please check that you have an NVIDIA GPU and installed a driver from http://www.nvidia.com/Download/index.aspx"
          ]
        }
      ],
      "source": [
        "from tqdm.auto import tqdm\n",
        "\n",
        "class Trainer:\n",
        "  def __init__(self, model, optimizer, loss_fn, train_loader, valid_loader, device):\n",
        "    self.model = model\n",
        "    self.optimizer = optimizer\n",
        "    self.loss_fn = torch.nn.BCELoss()\n",
        "    self.train_loader = train_loader\n",
        "    self.valid_loader = valid_loader\n",
        "\n",
        "    self.model.to(device)\n",
        "\n",
        "    self.best_valid_accuracy = 0\n",
        "    self.device = device\n",
        "\n",
        "    self.training_loss = []\n",
        "    self.training_acc = []\n",
        "    self.validation_loss = []\n",
        "    self.validation_acc = []\n",
        "\n",
        "  def save_model(self, path='imdb_sentiment_model.pt'):\n",
        "    torch.save({'model':self.model.state_dict(), 'optim':self.optimizer.state_dict()}, path)\n",
        "\n",
        "  def train_by_num_epoch(self, num_epochs):\n",
        "    for epoch in range(num_epochs):\n",
        "      self.model.train()\n",
        "      for batch in tqdm(self.train_loader, leave=False):\n",
        "        loss_value, acc = self._train_by_single_batch(batch)\n",
        "        self.training_loss.append(loss_value)\n",
        "        self.training_acc.append(acc)\n",
        "\n",
        "      self.model.eval()\n",
        "      validation_loss, validation_acc = self.validate()\n",
        "      self.validation_loss.append(validation_loss)\n",
        "      self.validation_acc.append(validation_acc)\n",
        "\n",
        "      print(f\"Epoch {epoch+1}, Training Loss: {loss_value:.4f}, Training Acc: {acc:.4f}, Validation Loss: {validation_loss:.4f}, Validation Acc: {validation_acc:.4f}\")\n",
        "      if validation_acc > self.best_valid_accuracy:\n",
        "        print(f\"Saving the model with best validation accuracy: Epoch {epoch+1}, Acc: {validation_acc:.4f} \")\n",
        "        self.save_model('imdb_sentiment_model_best.pt')\n",
        "      else:\n",
        "        self.save_model('imdb_sentiment_model_last.pt')\n",
        "      self.best_valid_accuracy = max(validation_acc, self.best_valid_accuracy)\n",
        "\n",
        "  def _get_accuracy(self, pred, target, threshold=0.5):\n",
        "    '''\n",
        "    This method calculates accuracy for given prediction and target\n",
        "\n",
        "    input:\n",
        "      pred (torch.Tensor): Prediction value for a given batch\n",
        "      target (torch.Tensor): Target value for a given batch\n",
        "      threshold (float): Threshold value for deciding whether the prediction is positive or negative. Default value is 0.5\n",
        "\n",
        "    output:\n",
        "      accuracy (float): Mean Accuracy value for every sample in a given batch\n",
        "\n",
        "    TODO: Complete this method using all the input arguments\n",
        "    '''\n",
        "    # Consider prediction as positive if >= 0.5\n",
        "    pred_label = (pred >= threshold).float()\n",
        "    # Calculate mean accuracy\n",
        "    accuracy = (pred_label == target).float().mean()\n",
        "\n",
        "    return accuracy.item()\n",
        "\n",
        "  def _get_loss_and_acc_from_single_batch(self, batch):\n",
        "    '''\n",
        "    This method calculates loss value for a given batch\n",
        "\n",
        "    batch (tuple): (batch_of_input_text, batch_of_label)\n",
        "\n",
        "    You have to use variables below:\n",
        "    self.model (SentimentModel/torch.nn.Module): A neural network model\n",
        "    self.loss_fn (function): function for calculating BCE loss for a given prediction and target\n",
        "    self.device (str): 'cuda' or 'cpu'\n",
        "    self._get_accuracy (function): function for calculating accuracy for a given prediction and target\n",
        "\n",
        "    output:\n",
        "      loss (torch.Tensor): Mean binary cross entropy value for every sample in the training batch\n",
        "      acc (float): Accuracy for the given batch\n",
        "    # CAUTION! The output loss has to be torch.Tensor that is backwardable, not a float value or numpy array\n",
        "\n",
        "    TODO: Complete this method\n",
        "    '''\n",
        "    # Split and move : input data (x) and labels (y)\n",
        "    x, y = batch\n",
        "    #to use same device\n",
        "    x, y = x.to(self.device), y.to(self.device)\n",
        "\n",
        "    pred = self.model(x)\n",
        "    loss = self.loss_fn(pred, y)\n",
        "    acc = self._get_accuracy(pred, y)\n",
        "\n",
        "    return loss, acc\n",
        "\n",
        "  def _train_by_single_batch(self, batch):\n",
        "    '''\n",
        "    This method updates self.model's parameter with a given batch\n",
        "\n",
        "    batch (tuple): (batch_of_input_text, batch_of_label)\n",
        "\n",
        "    You have to use methods and variables below:\n",
        "\n",
        "    self._get_loss_and_acc_from_single_batch (function): function for calculating loss value for a given batch\n",
        "    self.optimizer (torch.optim.adam.Adam): Adam optimizer that optimizes model's parameter\n",
        "\n",
        "    output:\n",
        "      loss (float): Mean binary cross entropy value for every sample in the training batch\n",
        "      acc (float): Mean accuracy for the given batch\n",
        "    The model's parameters, optimizer's steps has to be updated inside this method\n",
        "\n",
        "    TODO: Complete this method\n",
        "    '''\n",
        "    self.optimizer.zero_grad()  # Reset gradients\n",
        "    loss, acc = self._get_loss_and_acc_from_single_batch(batch)  # Get loss and accuracy for this batch\n",
        "    loss.backward()  # Backpropagate the error\n",
        "    self.optimizer.step()  # Update model parameters\n",
        "\n",
        "    return loss.item(), acc  # Return loss and accuracy as float\n",
        "\n",
        "  def validate(self, external_loader=None):\n",
        "    '''\n",
        "    This method calculates accuracy and loss for given data loader.\n",
        "    It can be used for validation step, or to get test set result\n",
        "\n",
        "    input:\n",
        "      data_loader: If there is no data_loader given, use self.valid_loader as default.\n",
        "\n",
        "    output:\n",
        "      validation_loss (float): Mean Binary Cross Entropy value for every sample in validation set\n",
        "      validation_accuracy (float): Mean Accuracy value for every sample in validation set\n",
        "\n",
        "    Use these methods:\n",
        "      self._get_loss_and_acc_from_single_batch (function): function for calculating loss value for a given batch\n",
        "\n",
        "    TODO: Complete this method\n",
        "    CAUTION: During validation, you can make it faster by not calculating gradient\n",
        "\n",
        "    '''\n",
        "    loader = external_loader if external_loader else self.valid_loader\n",
        "    self.model.eval()\n",
        "\n",
        "    total_loss, total_acc = 0, 0\n",
        "    count = 0\n",
        "    with torch.no_grad():\n",
        "      for batch in loader:\n",
        "        loss, acc = self._get_loss_and_acc_from_single_batch(batch)\n",
        "        total_loss += loss.item()\n",
        "        total_acc += acc\n",
        "        count += 1\n",
        "    return total_loss / count, total_acc / count\n",
        "\n",
        "    ### Don't change this part\n",
        "    if external_loader and isinstance(external_loader, DataLoader):\n",
        "      loader = external_loader\n",
        "      print('An arbitrary loader is used instead of Validation loader')\n",
        "    else:\n",
        "      loader = self.valid_loader\n",
        "\n",
        "    self.model.eval()\n",
        "\n",
        "    '''\n",
        "    Write your code from here, using loader, self.model, self.loss_fn.\n",
        "    '''\n",
        "\n",
        "    return\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\"\"\"\n",
        "Don't change this part\n",
        "\"\"\"\n",
        "model = SentimentModel(len(converter.idx2str), hidden_size=128, num_layers=3)\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
        "train_loader = DataLoader(trainset, batch_size=64, collate_fn=pack_collate, shuffle=True)\n",
        "valid_loader = DataLoader(validset, batch_size=128, collate_fn=pack_collate, shuffle=False)\n",
        "test_loader = DataLoader(testset, batch_size=128, collate_fn=pack_collate, shuffle=False)\n",
        "\n",
        "trainer =  Trainer(model, optimizer, get_binary_cross_entropy_loss, train_loader, valid_loader, device='cuda')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "782a04f7",
      "metadata": {
        "id": "782a04f7"
      },
      "source": [
        "#### Check the result\n",
        "- Check your implementation works correctly\n",
        "- Don't change the code below"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a8db5dae",
      "metadata": {
        "id": "a8db5dae"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "This code is to test trainer._train_by_single_batch\n",
        "\n",
        "If your code is implemented correctly, the loss will go down for this specific batch\n",
        "\"\"\"\n",
        "\n",
        "trainer.model.train()\n",
        "train_batch = next(iter(trainer.train_loader)) # get a batch from train_loader\n",
        "\n",
        "loss_track = []\n",
        "for _ in range(10):\n",
        "  loss_value, acc = trainer._train_by_single_batch(train_batch) # test the trainer\n",
        "  loss_track.append(loss_value)\n",
        "\n",
        "assert isinstance(loss_value, float) and loss_value > 0,  \"The return of trainer._train_by_single_batch has to be a single float value that is larger than 0\"\n",
        "print(f\"Loss value for 10 repetition for the same training batch is  {[f'{loss:.4f}' for loss in loss_track]}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e9dc9edf",
      "metadata": {
        "id": "e9dc9edf"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "This code is to test trainer.validate\n",
        "\"\"\"\n",
        "\n",
        "short_valid_loader = DataLoader(short_validset, batch_size=50, collate_fn=pack_collate)\n",
        "\n",
        "validation_loss, validation_acc = trainer.validate(short_valid_loader)\n",
        "assert isinstance(validation_loss, float) and isinstance(validation_acc, float), \"Both return value of trainer.validate has to be float\"\n",
        "assert validation_loss > 0, \"Validation Loss has to be larger than 1\"\n",
        "assert 0 <= validation_acc <= 1, \"Validation Acc has to be between 0 and 1\"\n",
        "\n",
        "print(f\"Valid loss: {validation_loss}, Accuracy: {validation_acc}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1ab7cd7a",
      "metadata": {
        "id": "1ab7cd7a"
      },
      "source": [
        "### Train the model with the completed Trainer\n",
        "- In your report, attach the result of following cells and describe the training result and test result\n",
        "    - Plot of training and validation loss/acc\n",
        "    - Result of your model on test set\n",
        "\n",
        "- [Optional] You can modify the code to train the model in different ways\n",
        "    - optimizer, batch_size, model_size, num_epcohs, etc"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "29853df0",
      "metadata": {
        "id": "29853df0",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "trainer.train_by_num_epoch(5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3987770b",
      "metadata": {
        "id": "3987770b"
      },
      "outputs": [],
      "source": [
        "'''\n",
        "Plot the result after the training\n",
        "'''\n",
        "\n",
        "plt.figure(figsize=(10,8))\n",
        "\n",
        "plt.subplot(4,1,1)\n",
        "plt.title(\"Training loss\")\n",
        "plt.plot(trainer.training_loss)\n",
        "\n",
        "plt.subplot(4,1,2)\n",
        "plt.title(\"Training accuracy\")\n",
        "plt.plot(trainer.training_acc)\n",
        "\n",
        "plt.subplot(4,1,3)\n",
        "plt.title(\"Validation loss by epoch\")\n",
        "plt.plot(trainer.validation_loss)\n",
        "\n",
        "plt.subplot(4,1,4)\n",
        "plt.title(\"Validation accuracy by epoch\")\n",
        "plt.plot(trainer.validation_acc)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e1eec925",
      "metadata": {
        "id": "e1eec925"
      },
      "outputs": [],
      "source": [
        "'''\n",
        "Get the test result\n",
        "'''\n",
        "\n",
        "test_loss, test_acc = trainer.validate(test_loader)\n",
        "\n",
        "print(f\"Test Loss: {test_loss}, Test Accuracy: {test_acc}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "816b4469",
      "metadata": {
        "id": "816b4469"
      },
      "source": [
        "### Paste your code to ``NLP_Assignment_2.py`` file\n",
        "- Copy and paste your completed code to ``NLP_Assignment_2.py`` file\n",
        "- You can check your code is correct by running the following cell\n",
        "  - In my test, my last print was\n",
        "    - Valid loss: 0.6931650042533875, Accuracy: 0.5\n",
        "    - Epoch 1, Training Loss: 0.6802, Training Acc: 0.6875, Validation Loss: 0.6942, Validation Acc: 0.5100\n",
        "    - Saving the model with best validation accuracy: Epoch 1, Acc: 0.5100\n",
        "    - Last 5 Training loss: [0.6907180547714233, 0.6797541379928589, 0.7054318189620972, 0.6722485423088074, 0.6801780462265015]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "85fa99b7",
      "metadata": {
        "id": "85fa99b7"
      },
      "outputs": [],
      "source": [
        "!python3 NLP_Assignment_2.py"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "672a2ea1",
      "metadata": {
        "id": "672a2ea1"
      },
      "source": [
        "## Problem 6: Analyze the Prediction of the model\n",
        "- In this problem, you have to anlayze the prediction of the model\n",
        "    - You can select among the two models\n",
        "        - Trainer is designed to save two models\n",
        "        - ``imdb_sentiment_model_last.pt`` contains the model weights after the last training epoch\n",
        "        - ``imdb_sentiment_model_best.pt`` contains the model weights after the training epoch with the best validation accuracy\n",
        "     \n",
        "- If you failed to train your model by solving the previous problems, you can download the model\n",
        "- In your report, describe your analysis on how the trained model works on the text\n",
        "    - What is the main criteria for model to decide whether the review is positive or negative?\n",
        "    - When does it make mistakes? When does it make nice predictions?\n",
        "    - Does the converted input text has enough information to classify text compared to the original text?\n",
        "        - Do you see any problems in tokenizing or using UNKNOWN?\n",
        "- You can write the analysis by using only the Test Set samples, or using your own review texts\n",
        "    \n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fe75c333",
      "metadata": {
        "id": "fe75c333"
      },
      "source": [
        "#### Download Model (if you have failed to trained your own)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "22533d5d",
      "metadata": {
        "id": "22533d5d"
      },
      "outputs": [],
      "source": [
        "'''\n",
        "If you failed to train your own model, you can download the \"imdb_sentiment_model_best_pretrained.pt\" in CyberCampus and upload it to the colab or your workspace.\n",
        "'''\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5aeaf292",
      "metadata": {
        "id": "5aeaf292"
      },
      "source": [
        "#### Load Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bf4f40e9",
      "metadata": {
        "id": "bf4f40e9"
      },
      "outputs": [],
      "source": [
        "model = SentimentModel(len(converter.idx2str), 128, 3) # You have to use the same model architecture as the one you used for training\n",
        "your_model_pt_path = 'imdb_sentiment_model_last.pt' # or imdb_sentiment_model_best.pt, or imdb_sentiment_model_best_pretrained.pt etc\n",
        "model.load_state_dict(torch.load(your_model_pt_path, map_location='cpu')['model']) # Or imdb_sentiment_model_best.pt"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0cff2d4d",
      "metadata": {
        "id": "0cff2d4d"
      },
      "source": [
        "#### Check the largest error cases from the Test Set\n",
        "- Following code will print out the error case with the largest errors\n",
        "    - Or data sample with smallest error, with ``largest_loss=False``"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "02737842",
      "metadata": {
        "id": "02737842"
      },
      "outputs": [],
      "source": [
        "nl = '\\n'\n",
        "def sort_data_idx_by_loss(model, data_loader, device='cuda'):\n",
        "  assert isinstance(data_loader.sampler, torch.utils.data.sampler.SequentialSampler)\n",
        "  model.eval()\n",
        "  model.to(device)\n",
        "  entire_loss = []\n",
        "  entire_pred = []\n",
        "  loss_fn = torch.nn.BCELoss(reduction='none')\n",
        "  with torch.no_grad():\n",
        "    for batch in tqdm(data_loader):\n",
        "      x, y =batch\n",
        "      pred = model(x.to(device))\n",
        "      loss = loss_fn(pred, y.to(device))\n",
        "      entire_loss += loss.tolist()\n",
        "      entire_pred += pred.tolist()\n",
        "  sorted_indices = sorted(range(len(entire_loss)),key=entire_loss.__getitem__)\n",
        "  return sorted_indices, entire_loss, entire_pred\n",
        "\n",
        "def print_top_k_loss_case(data_loader, sorted_indices, entire_loss, entire_pred, k=10, descending=True):\n",
        "  if descending:\n",
        "    sorted_indices = reversed(sorted_indices[-k:])\n",
        "  else:\n",
        "    sorted_indices = sorted_indices[:k]\n",
        "\n",
        "  texts = []\n",
        "  for i, idx in enumerate(sorted_indices):\n",
        "    data_sample = data_loader.dataset[idx]\n",
        "    conv_text = ' '.join(converter(converter(data_sample[0])))\n",
        "    orig_text = read_txt(data_loader.dataset.paths[idx])\n",
        "    texts.append({'converted': conv_text, 'original': orig_text})\n",
        "    print(f\" {i}. Sample index: {idx} - Loss: {entire_loss[idx]:.4f}, Model Prediction: {entire_pred[idx]:.4f}, Correct Label: {data_sample[1]} \\\n",
        "          {nl}  Converted Text: {conv_text} {nl}  Original Text: {orig_text} {nl}\")\n",
        "  return texts\n",
        "\n",
        "\n",
        "'''\n",
        "This will calculate loss for each sample in the test set\n",
        "'''\n",
        "sorted_indices, entire_loss, entire_pred =  sort_data_idx_by_loss(model, test_loader, device='cuda')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e92c8895",
      "metadata": {
        "id": "e92c8895"
      },
      "outputs": [],
      "source": [
        "'''\n",
        "This will print out top-k most incorrect prediction on test set\n",
        "'''\n",
        "texts = print_top_k_loss_case(test_loader, sorted_indices, entire_loss, entire_pred, k=10, descending=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9291a4e7",
      "metadata": {
        "id": "9291a4e7"
      },
      "outputs": [],
      "source": [
        "'''\n",
        "This will print out top-k most correct prediction on test set\n",
        "'''\n",
        "\n",
        "texts = print_top_k_loss_case(test_loader, sorted_indices, entire_loss, entire_pred, k=10, descending=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3c078561",
      "metadata": {
        "id": "3c078561"
      },
      "source": [
        "#### Test with your own text input"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d77e9711",
      "metadata": {
        "id": "d77e9711"
      },
      "outputs": [],
      "source": [
        "def estimate_sentiment_of_given_txt(model, input_text):\n",
        "  model.cpu()\n",
        "  model.eval()\n",
        "  tokenizer = trainset.tokenizer\n",
        "  your_text_in_token = tokenizer(input_text)\n",
        "  model_input = pack_sequence([torch.tensor(converter(your_text_in_token), dtype=torch.long)])\n",
        "  prediction = model(model_input).squeeze()\n",
        "\n",
        "  return prediction\n",
        "\n",
        "\n",
        "your_text = \"\"\"\n",
        "    This movie is terrific\n",
        "\"\"\"\n",
        "\n",
        "estimate_sentiment_of_given_txt(model, your_text)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}