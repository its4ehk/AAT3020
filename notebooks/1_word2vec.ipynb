{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/its4ehk/AAT3020/blob/main/notebooks/1_word2vec.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6FZ2ZSwcyhDQ"
      },
      "source": [
        "# Word2Vec Implementation from Scratch\n",
        "\n",
        "This notebook demonstrates how to implement the Word2Vec algorithm from scratch using PyTorch. We'll use the first Harry Potter book as our corpus to train word embeddings.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pDRbSHQIyhDS"
      },
      "source": [
        "## 1. Setting Up the Environment\n",
        "\n",
        "First, we need to import the necessary libraries:\n",
        "- `torch` and `torch.nn` for tensor operations and neural network functionality\n",
        "- `string` for string manipulations (removing punctuation)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "L9BA5Lg2QRMr"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import string\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EA7ERi6CyhDS"
      },
      "source": [
        "## 2. Getting the Text Data\n",
        "\n",
        "We'll download the first Harry Potter book to use as our corpus."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tEaaz_s0QRMs",
        "outputId": "ad1a498a-c7c4-4a22-bdee-e2d0a85eeda0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2025-03-20 04:46:31--  https://raw.githubusercontent.com/amephraim/nlp/master/texts/J.%20K.%20Rowling%20-%20Harry%20Potter%201%20-%20Sorcerer's%20Stone.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 439742 (429K) [text/plain]\n",
            "Saving to: ‘J. K. Rowling - Harry Potter 1 - Sorcerer's Stone.txt’\n",
            "\n",
            "\r          J. K. Row   0%[                    ]       0  --.-KB/s               \rJ. K. Rowling - Har 100%[===================>] 429.44K  --.-KB/s    in 0.05s   \n",
            "\n",
            "2025-03-20 04:46:31 (9.03 MB/s) - ‘J. K. Rowling - Harry Potter 1 - Sorcerer's Stone.txt’ saved [439742/439742]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!wget \"https://raw.githubusercontent.com/amephraim/nlp/master/texts/J.%20K.%20Rowling%20-%20Harry%20Potter%201%20-%20Sorcerer's%20Stone.txt\"\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PT6qumaDyhDT"
      },
      "source": [
        "## 3. Text Preprocessing\n",
        "\n",
        "Before we can use the text data, we need to preprocess it:\n",
        "- Remove punctuation\n",
        "- Convert text to lowercase\n",
        "- Split text into tokens (words)\n",
        "\n",
        "This function will help us clean and tokenize the text."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "CUsXJYlIQRMs"
      },
      "outputs": [],
      "source": [
        "def remove_punctuation(x):\n",
        "  return x.translate(''.maketrans('', '', string.punctuation))\n",
        "\n",
        "def make_tokenized_corpus(corpus):\n",
        "  out= [ [y.lower() for y in remove_punctuation(sentence).split(' ') if y] for sentence in corpus]\n",
        "  return [x for x in out if x!=[]]\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ptv8VRGXyhDT"
      },
      "source": [
        "## 4. Loading and Formatting the Text\n",
        "\n",
        "Now we'll load the text file, replace some special characters, and split the text into sentences.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "Ry1o-F-bQRMs"
      },
      "outputs": [],
      "source": [
        "with open(\"J. K. Rowling - Harry Potter 1 - Sorcerer's Stone.txt\", 'r') as f:\n",
        "  strings = f.readlines()\n",
        "sample_text = \"\".join(strings).replace('\\n', ' ').replace('Mr.', 'mr').replace('Mrs.', 'mrs').split('. ')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p-mCXyJuyhDT"
      },
      "source": [
        "Let's tokenize the text using our preprocessing function `make_tokenized_corpus`:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "Ne-pUaxSQRMs"
      },
      "outputs": [],
      "source": [
        "# Corpus is a list of list of strings (words)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "corpus = make_tokenized_corpus(sample_text)\n",
        "\n",
        "corpus[:5]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Sx6UoEHjylS1",
        "outputId": "38a75443-9351-4a64-bc22-551cc1dd3905"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[['harry',\n",
              "  'potter',\n",
              "  'and',\n",
              "  'the',\n",
              "  'sorcerers',\n",
              "  'stone',\n",
              "  'chapter',\n",
              "  'one',\n",
              "  'the',\n",
              "  'boy',\n",
              "  'who',\n",
              "  'lived',\n",
              "  'mr',\n",
              "  'and',\n",
              "  'mrs',\n",
              "  'dursley',\n",
              "  'of',\n",
              "  'number',\n",
              "  'four',\n",
              "  'privet',\n",
              "  'drive',\n",
              "  'were',\n",
              "  'proud',\n",
              "  'to',\n",
              "  'say',\n",
              "  'that',\n",
              "  'they',\n",
              "  'were',\n",
              "  'perfectly',\n",
              "  'normal',\n",
              "  'thank',\n",
              "  'you',\n",
              "  'very',\n",
              "  'much'],\n",
              " ['they',\n",
              "  'were',\n",
              "  'the',\n",
              "  'last',\n",
              "  'people',\n",
              "  'youd',\n",
              "  'expect',\n",
              "  'to',\n",
              "  'be',\n",
              "  'involved',\n",
              "  'in',\n",
              "  'anything',\n",
              "  'strange',\n",
              "  'or',\n",
              "  'mysterious',\n",
              "  'because',\n",
              "  'they',\n",
              "  'just',\n",
              "  'didnt',\n",
              "  'hold',\n",
              "  'with',\n",
              "  'such',\n",
              "  'nonsense'],\n",
              " ['mr',\n",
              "  'dursley',\n",
              "  'was',\n",
              "  'the',\n",
              "  'director',\n",
              "  'of',\n",
              "  'a',\n",
              "  'firm',\n",
              "  'called',\n",
              "  'grunnings',\n",
              "  'which',\n",
              "  'made',\n",
              "  'drills'],\n",
              " ['he',\n",
              "  'was',\n",
              "  'a',\n",
              "  'big',\n",
              "  'beefy',\n",
              "  'man',\n",
              "  'with',\n",
              "  'hardly',\n",
              "  'any',\n",
              "  'neck',\n",
              "  'although',\n",
              "  'he',\n",
              "  'did',\n",
              "  'have',\n",
              "  'a',\n",
              "  'very',\n",
              "  'large',\n",
              "  'mustache'],\n",
              " ['mrs',\n",
              "  'dursley',\n",
              "  'was',\n",
              "  'thin',\n",
              "  'and',\n",
              "  'blonde',\n",
              "  'and',\n",
              "  'had',\n",
              "  'nearly',\n",
              "  'twice',\n",
              "  'the',\n",
              "  'usual',\n",
              "  'amount',\n",
              "  'of',\n",
              "  'neck',\n",
              "  'which',\n",
              "  'came',\n",
              "  'in',\n",
              "  'very',\n",
              "  'useful',\n",
              "  'as',\n",
              "  'she',\n",
              "  'spent',\n",
              "  'so',\n",
              "  'much',\n",
              "  'of',\n",
              "  'her',\n",
              "  'time',\n",
              "  'craning',\n",
              "  'over',\n",
              "  'garden',\n",
              "  'fences',\n",
              "  'spying',\n",
              "  'on',\n",
              "  'the',\n",
              "  'neighbors']]"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rLBY2P7_yhDU"
      },
      "source": [
        "## 5. Creating Context Word Pairs\n",
        "\n",
        "A key concept in Word2Vec is learning from context. We need to create pairs of words that appear near each other in the text. We'll use a sliding window approach to create these pairs.\n",
        "\n",
        "For example, with the window size of 2, for the word \"to\" in the sentence \"they were the last people youd expect to be involved...\", we would create pairs with:\n",
        "- (\"to\", \"expect\")\n",
        "- (\"to\", \"be\")\n",
        "- (\"to\", \"involved\")\n",
        "- (\"to\", \"in\")\n",
        "\n",
        "These pairs will be our training data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-UxJwTAacWfP",
        "outputId": "febfba6e-1c86-4608-e3ec-8a2a3e3ba52c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 4682/4682 [00:00<00:00, 29486.57it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Length of word_pairs is 282372\n",
            "First 5 example of word_pairs is [('harry', 'potter'), ('harry', 'and'), ('potter', 'harry'), ('potter', 'and'), ('potter', 'the')]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "from tqdm import tqdm\n",
        "\n",
        "sample_sentence = ['they', 'were', 'the', 'last', 'people', 'youd', 'expect', 'to', 'be', 'involved', 'in', 'anything', 'strange', 'or', 'mysterious', 'because', 'they', 'just', 'didnt', 'hold', 'with', 'such', 'nonsense']\n",
        "\n",
        "word_pairs = []\n",
        "window_size = 2\n",
        "\n",
        "for sample_sentence in tqdm(corpus):\n",
        "  for cur_idx, center_word in enumerate(sample_sentence):\n",
        "    window_begin = max(cur_idx - window_size, 0)\n",
        "    window_end = min(cur_idx + window_size + 1, len(sample_sentence))\n",
        "    # for context_word in sample_sentence[window_begin:window_end]:\n",
        "    #   # if center_word == context_word: continue\n",
        "    #   word_pairs.append( (center_word, context_word))\n",
        "    for j in range(window_begin, window_end):\n",
        "      if cur_idx == j: continue\n",
        "      word_pairs.append( (center_word, sample_sentence[j]))\n",
        "\n",
        "print(f\"\\nLength of word_pairs is {len(word_pairs)}\")\n",
        "print(f\"First 5 example of word_pairs is {word_pairs[:5]}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WD-SN32cyhDU"
      },
      "source": [
        "## 6. Building the Vocabulary\n",
        "\n",
        "To work with word vectors, we need to create a vocabulary that maps each unique word to an index. We'll also filter out rare words that appear less than a certain number of times in the corpus.\n",
        "\n",
        "### 6.1 Collecting All Words\n",
        "\n",
        "First, let's collect all words in our corpus:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "htinJiMPkkRE"
      },
      "outputs": [],
      "source": [
        "# we have to make vocabulary\n",
        "sentence = corpus[0]\n",
        "entire_words = []\n",
        "\n",
        "for sentence in corpus:\n",
        "  for word in sentence:\n",
        "    entire_words.append(word)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j_68LD6IyhDU"
      },
      "source": [
        "\n",
        "### 6.2 Finding Unique Words\n",
        "\n",
        "Now, let's find the unique words in our corpus:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ERBFCjeslgDe",
        "outputId": "1a4fbcdc-5e77-464b-c577-f074f80b32f6"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "6038"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ],
      "source": [
        "# we have to get the \"unique\" item among total words\n",
        "\n",
        "unique_words = set(entire_words)\n",
        "len(unique_words)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H7oqHw4dyhDU"
      },
      "source": [
        "### 6.3 Converting to a List and Sorting\n",
        "\n",
        "We'll convert the set of unique words to a sorted list:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "fDJNrHdhl_dk",
        "outputId": "16cbc713-e20d-447b-a7f3-f9feb193926f"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\the'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 11
        }
      ],
      "source": [
        "# vocab_set[0] # set is not subscriptable because it has no order\n",
        "\n",
        "unique_words = sorted(list(unique_words))\n",
        "unique_words[0]\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gyPBBsJCyhDU"
      },
      "source": [
        "### 6.4 Filtering by Frequency\n",
        "\n",
        "Now, let's filter out rare words that occur less than a specified number of times:\n",
        "- We can use the `Counter` class from the `collections` module to count the frequency of each word in the corpus.\n",
        "- Caution on `alist.sort()` will return `None`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wOkBSjrkmNE4",
        "outputId": "e31ce662-66f9-4b28-c580-fd2664718a6f"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['a',\n",
              " 'able',\n",
              " 'abou',\n",
              " 'about',\n",
              " 'above',\n",
              " 'across',\n",
              " 'added',\n",
              " 'afford',\n",
              " 'afraid',\n",
              " 'after']"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ],
      "source": [
        "# how can we filter the vocab by its frequency?\n",
        "filtered_vocab = None\n",
        "# you can use word counter as dictionary\n",
        "# In python dictionary, dict.keys() gives keys, and dict.values() give values,\n",
        "# dict.items() give (key, value)\n",
        "\n",
        "from collections import Counter\n",
        "word_counter = Counter(entire_words)\n",
        "word_counter.most_common(10)\n",
        "word_counter['harry']\n",
        "\n",
        "threshold = 5\n",
        "filtered_vocab = []\n",
        "\n",
        "for key, value in word_counter.items():\n",
        "  if value > threshold:\n",
        "    filtered_vocab.append(key)\n",
        "\n",
        "filtered_vocab.sort()\n",
        "filtered_vocab[:10]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gfez7UfYyhDU"
      },
      "source": [
        "## 7. Filtering Word Pairs\n",
        "\n",
        "Now that we have our filtered vocabulary, we need to filter our word pairs to only include words that are in our vocabulary:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XUS6U7y7opUp",
        "outputId": "2c3ddff5-e6c2-48fa-ce6c-2e366d250e72"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 282372/282372 [00:00<00:00, 1946749.96it/s]\n"
          ]
        }
      ],
      "source": [
        "# Filter the word_pairs using the vocab\n",
        "# word_pairs, filtered_vocab\n",
        "# word_pairs is a list of [word_a, word_b]\n",
        "\n",
        "filtered_word_pairs = []\n",
        "vocab_set = set(filtered_vocab)\n",
        "\n",
        "for pair in tqdm(word_pairs):\n",
        "  a, b = pair\n",
        "  if a in vocab_set and b in vocab_set:\n",
        "    filtered_word_pairs.append(pair)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "U-o4UucOrcem",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a6fad458-a995-4062-de53-7c214bcc89de"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('harry', 'potter')"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ],
      "source": [
        "# implement same algorithm with list comprehension\n",
        "\n",
        "filtered_word_pairs = [pair for pair in word_pairs if pair[0] in vocab_set and pair[1] in vocab_set]\n",
        "filtered_word_pairs[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Uz_8ch59ps_P",
        "outputId": "b8314d12-0f88-4202-e645-c03bb19a02bf"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(226846, 282372)"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ],
      "source": [
        "len(filtered_word_pairs), len(word_pairs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fGqUhV1ayhDU"
      },
      "source": [
        "## 8. Converting Words to Indices\n",
        "\n",
        "For efficiency, we'll convert our words to indices according to their position in our vocabulary:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VFJfhOznqyi-",
        "outputId": "667d3df9-fade-40fb-d5b3-9cabca67e7b6"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "527"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ],
      "source": [
        "# convert word into index of vocab\n",
        "filtered_vocab.index('harry')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mr7Ov52nyhDU"
      },
      "source": [
        "This is inefficient because `list.index()` has to scan the list every time. Let's use a dictionary for faster lookups:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2D8n16VitIHP",
        "outputId": "0f1b19b8-80d7-46c0-a6bd-baa1997dac6e"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "527"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ],
      "source": [
        "# we can make it faster\n",
        "# use dictionary to find the index of string\n",
        "word2idx = dict()\n",
        "for idx, word in enumerate(filtered_vocab):\n",
        "  word2idx[word] = idx\n",
        "word2idx['harry']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GG9ESVqLyhDV"
      },
      "source": [
        "Now, let's convert our word pairs to index pairs more efficiently:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "utXemuOgt8-o",
        "outputId": "53d3bb90-57fc-41e4-acdb-14908cadade0"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(527, 953)"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ],
      "source": [
        "index_pairs = [(word2idx[pair[0]], word2idx[pair[1]]) for pair in filtered_word_pairs]\n",
        "index_pairs[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "KgQ_oSNGuZAd",
        "outputId": "2354c1c4-abcd-41d7-891b-cfad6cb61a5d"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'harry'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 19
        }
      ],
      "source": [
        "# Why we don't need idx2tok?\n",
        "\n",
        "filtered_vocab[527]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "waHqgJANyhDV"
      },
      "source": [
        "## 9. Creating Initial Word Vectors\n",
        "\n",
        "Now we'll create random vectors for each word in our vocabulary. These vectors will be adjusted during training:\n",
        "- We can use `torch.randn` to create random vectors that follow normal distribution."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ygV93qzDu4Ls",
        "outputId": "f028ac1f-d4d3-4ca3-9176-15ab22c49665"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[ 0.0125, -0.0370, -0.0671,  ...,  0.0198, -0.0279,  0.0918],\n",
              "        [ 0.1049,  0.0743,  0.0598,  ...,  0.2685,  0.0057,  0.0385],\n",
              "        [-0.0101,  0.0855,  0.0920,  ...,  0.0360,  0.0085,  0.1022],\n",
              "        ...,\n",
              "        [-0.0077, -0.1131,  0.1762,  ...,  0.0134,  0.1058, -0.0752],\n",
              "        [ 0.0978, -0.0145, -0.0401,  ...,  0.1495, -0.2357,  0.1673],\n",
              "        [-0.0581,  0.1191,  0.0291,  ..., -0.0869,  0.1212,  0.0894]])"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ],
      "source": [
        "# we have to make random vectors for each word in the vocab\n",
        "# we also have to decide the dimension of the vector\n",
        "\n",
        "dim = 100\n",
        "vocab_size = len(filtered_vocab)\n",
        "\n",
        "word_vectors = torch.randn(vocab_size, dim) / 10\n",
        "word_vectors"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vmZcT53rvwW2",
        "outputId": "bb86bc38-bcc0-463f-deb1-b2ad8a05640e"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([ 0.0881, -0.0740, -0.0561, -0.1266,  0.1663, -0.0900,  0.0298,  0.0351,\n",
              "         0.1492, -0.0421, -0.0466,  0.1441,  0.2179, -0.0794,  0.0028, -0.0967,\n",
              "         0.0440, -0.0257, -0.0207, -0.0489,  0.0107,  0.1389, -0.0006,  0.0966,\n",
              "         0.0843,  0.0082, -0.0047,  0.0521,  0.1083, -0.1749, -0.0258, -0.0597,\n",
              "        -0.0433,  0.0165, -0.0004, -0.0764,  0.1586, -0.1006, -0.0894, -0.0663,\n",
              "         0.0185, -0.1242, -0.0476,  0.2011, -0.1548, -0.1597,  0.0778, -0.1771,\n",
              "         0.0484,  0.0512,  0.0447, -0.0294,  0.0595, -0.0282, -0.0755, -0.0997,\n",
              "        -0.0241,  0.1902, -0.0935, -0.0224, -0.0387,  0.1644, -0.0845, -0.0395,\n",
              "        -0.0663,  0.0648,  0.0547,  0.0847, -0.0382,  0.0851, -0.0809,  0.0153,\n",
              "         0.0265, -0.0220, -0.0864,  0.0620, -0.0104, -0.0424, -0.0993, -0.1280,\n",
              "         0.0431,  0.0799,  0.0244,  0.1304,  0.0051,  0.0577, -0.0211,  0.0904,\n",
              "        -0.0234,  0.0676, -0.0988,  0.0928, -0.0723,  0.2078, -0.0628, -0.0143,\n",
              "        -0.1006, -0.0194, -0.0167, -0.1734])"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ],
      "source": [
        "# what is the vector for harry?\n",
        "\n",
        "word_vectors[word2idx['harry']]\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LLdvJrdKyhDV"
      },
      "source": [
        "## 10. Understanding Word Relationships with Dot Products\n",
        "\n",
        "The core of Word2Vec is using dot products to measure relationships between words. Let's explore this concept:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "J4bEKFvUxbVv"
      },
      "outputs": [],
      "source": [
        "torch.set_printoptions(sci_mode=False) # Do this to avoid scientific notation\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VaCMWmD6Suhy"
      },
      "source": [
        "## Dot Product\n",
        "- Assume we have two vectors $a$ and $b$.\n",
        "  - $a = [a_1, a_2, a_3, a_4, ..., a_n]$\n",
        "  - $b = [b_1, b_2, b_3, b_4, ..., b_n]$\n",
        "- $a \\cdot b$ = $\\sum _{i=1}^n a_ib_i$  = $a_1b_1 + a_2b_2 + a_3b_3 + a_4b_4 + ... + a_nb_n$\n",
        "\n",
        "Let's calculate the dot product between \"harry\" and \"potter\":\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gsse-jUrw6c2",
        "outputId": "95db52ed-ea71-4462-b15b-cf71adefc282"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(0.0447)"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ],
      "source": [
        "# calculate P(potter|harry)\n",
        "harry = word_vectors[word2idx['harry']]\n",
        "potter = word_vectors[word2idx['potter']]\n",
        "dot_product_value_between_potter_harry = sum(harry * potter)\n",
        "dot_product_value_between_potter_harry"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wZrLEY36yBNZ",
        "outputId": "9323df37-eba4-4999-907e-48566d185af4",
        "collapsed": true
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'a': tensor(-0.0004),\n",
              " 'able': tensor(0.0153),\n",
              " 'abou': tensor(0.0524),\n",
              " 'about': tensor(-0.0920),\n",
              " 'above': tensor(0.0096),\n",
              " 'across': tensor(-0.0128),\n",
              " 'added': tensor(-0.0567),\n",
              " 'afford': tensor(0.0608),\n",
              " 'afraid': tensor(0.0495),\n",
              " 'after': tensor(-0.0409),\n",
              " 'afternoon': tensor(0.0808),\n",
              " 'again': tensor(0.0114),\n",
              " 'against': tensor(-0.0107),\n",
              " 'ages': tensor(-0.0202),\n",
              " 'ago': tensor(-0.0034),\n",
              " 'agreed': tensor(-0.1055),\n",
              " 'ah': tensor(0.0491),\n",
              " 'ahead': tensor(-0.0417),\n",
              " 'air': tensor(-0.2322),\n",
              " 'albus': tensor(0.0376),\n",
              " 'alive': tensor(0.0824),\n",
              " 'all': tensor(0.1443),\n",
              " 'alley': tensor(-0.0106),\n",
              " 'allowed': tensor(0.0832),\n",
              " 'almost': tensor(0.0170),\n",
              " 'alone': tensor(0.0378),\n",
              " 'along': tensor(-0.1158),\n",
              " 'already': tensor(0.0009),\n",
              " 'also': tensor(-0.0746),\n",
              " 'although': tensor(0.0428),\n",
              " 'always': tensor(-0.0535),\n",
              " 'am': tensor(-0.0102),\n",
              " 'an': tensor(-0.0681),\n",
              " 'and': tensor(0.0537),\n",
              " 'angrily': tensor(-0.0911),\n",
              " 'angry': tensor(-0.1330),\n",
              " 'another': tensor(-0.0369),\n",
              " 'answer': tensor(0.0178),\n",
              " 'any': tensor(-0.1953),\n",
              " 'anymore': tensor(0.0257),\n",
              " 'anyone': tensor(-0.0145),\n",
              " 'anythin': tensor(-0.1196),\n",
              " 'anything': tensor(-0.0199),\n",
              " 'anyway': tensor(-0.0531),\n",
              " 'anywhere': tensor(-0.0199),\n",
              " 'apart': tensor(-0.1142),\n",
              " 'appeared': tensor(-0.1147),\n",
              " 'are': tensor(-0.1500),\n",
              " 'arent': tensor(-0.0635),\n",
              " 'arm': tensor(0.0360),\n",
              " 'armor': tensor(-0.0808),\n",
              " 'arms': tensor(-0.0370),\n",
              " 'around': tensor(0.1179),\n",
              " 'arrived': tensor(-0.2182),\n",
              " 'arts': tensor(-0.1315),\n",
              " 'as': tensor(0.0319),\n",
              " 'ask': tensor(-0.0433),\n",
              " 'asked': tensor(0.0559),\n",
              " 'asking': tensor(-0.0926),\n",
              " 'asleep': tensor(-0.0208),\n",
              " 'at': tensor(0.0520),\n",
              " 'attention': tensor(0.1087),\n",
              " 'aunt': tensor(0.0125),\n",
              " 'awake': tensor(0.1469),\n",
              " 'away': tensor(-0.0624),\n",
              " 'baby': tensor(0.0463),\n",
              " 'back': tensor(-0.0762),\n",
              " 'backward': tensor(0.0292),\n",
              " 'bacon': tensor(-0.1142),\n",
              " 'bad': tensor(-0.0036),\n",
              " 'bag': tensor(0.0190),\n",
              " 'ball': tensor(0.0574),\n",
              " 'balls': tensor(-0.0522),\n",
              " 'bane': tensor(0.0751),\n",
              " 'barrier': tensor(0.0590),\n",
              " 'be': tensor(-0.0513),\n",
              " 'beans': tensor(-0.0297),\n",
              " 'beard': tensor(0.0376),\n",
              " 'became': tensor(-0.0639),\n",
              " 'because': tensor(0.0593),\n",
              " 'become': tensor(0.0656),\n",
              " 'bed': tensor(0.1879),\n",
              " 'bedroom': tensor(-0.0931),\n",
              " 'been': tensor(-0.0067),\n",
              " 'before': tensor(0.0006),\n",
              " 'began': tensor(-0.1019),\n",
              " 'behind': tensor(0.0933),\n",
              " 'being': tensor(0.0650),\n",
              " 'believe': tensor(-0.0470),\n",
              " 'below': tensor(-0.0079),\n",
              " 'beneath': tensor(-0.0875),\n",
              " 'bent': tensor(-0.1533),\n",
              " 'best': tensor(0.0246),\n",
              " 'bet': tensor(-0.0162),\n",
              " 'better': tensor(0.0842),\n",
              " 'between': tensor(0.0037),\n",
              " 'big': tensor(0.0779),\n",
              " 'bill': tensor(-0.1650),\n",
              " 'bin': tensor(-0.0098),\n",
              " 'binoculars': tensor(-0.1138),\n",
              " 'birthday': tensor(-0.0311),\n",
              " 'bit': tensor(0.0113),\n",
              " 'black': tensor(-0.0523),\n",
              " 'blankets': tensor(0.0277),\n",
              " 'blew': tensor(-0.0081),\n",
              " 'blood': tensor(-0.0862),\n",
              " 'bloody': tensor(-0.0177),\n",
              " 'bludger': tensor(-0.0926),\n",
              " 'bludgers': tensor(-0.0690),\n",
              " 'blue': tensor(0.0845),\n",
              " 'board': tensor(-0.0845),\n",
              " 'boat': tensor(-0.1091),\n",
              " 'boats': tensor(0.1321),\n",
              " 'body': tensor(0.1745),\n",
              " 'book': tensor(-0.0830),\n",
              " 'books': tensor(0.1859),\n",
              " 'both': tensor(-0.0126),\n",
              " 'bottle': tensor(-0.0982),\n",
              " 'bottles': tensor(0.1627),\n",
              " 'bottom': tensor(0.0377),\n",
              " 'bought': tensor(-0.0024),\n",
              " 'bowed': tensor(0.1011),\n",
              " 'box': tensor(0.0045),\n",
              " 'boy': tensor(-0.1517),\n",
              " 'boys': tensor(0.1420),\n",
              " 'branches': tensor(0.0634),\n",
              " 'brave': tensor(0.0039),\n",
              " 'break': tensor(0.0094),\n",
              " 'breakfast': tensor(-0.0406),\n",
              " 'breaking': tensor(-0.0453),\n",
              " 'breath': tensor(0.0057),\n",
              " 'breathing': tensor(0.1098),\n",
              " 'bright': tensor(0.0190),\n",
              " 'brilliant': tensor(-0.0222),\n",
              " 'broke': tensor(0.0002),\n",
              " 'broken': tensor(0.0110),\n",
              " 'broom': tensor(0.0455),\n",
              " 'brooms': tensor(0.1625),\n",
              " 'broomstick': tensor(-0.1575),\n",
              " 'broomsticks': tensor(0.0217),\n",
              " 'brother': tensor(-0.0750),\n",
              " 'brothers': tensor(0.0752),\n",
              " 'brought': tensor(-0.1193),\n",
              " 'brown': tensor(-0.1694),\n",
              " 'burst': tensor(0.0429),\n",
              " 'business': tensor(-0.0259),\n",
              " 'busy': tensor(0.0258),\n",
              " 'but': tensor(-0.0379),\n",
              " 'buy': tensor(0.0884),\n",
              " 'by': tensor(0.0677),\n",
              " 'cake': tensor(-0.0097),\n",
              " 'cakes': tensor(0.0148),\n",
              " 'call': tensor(-0.0796),\n",
              " 'called': tensor(0.0202),\n",
              " 'came': tensor(0.0515),\n",
              " 'can': tensor(-0.1635),\n",
              " 'cant': tensor(-0.1030),\n",
              " 'car': tensor(0.1015),\n",
              " 'card': tensor(-0.1989),\n",
              " 'care': tensor(0.1045),\n",
              " 'careful': tensor(0.0079),\n",
              " 'carefully': tensor(0.0905),\n",
              " 'carried': tensor(-0.1450),\n",
              " 'carrying': tensor(-0.1168),\n",
              " 'cart': tensor(-0.0644),\n",
              " 'case': tensor(0.0238),\n",
              " 'castle': tensor(0.0508),\n",
              " 'cat': tensor(0.1543),\n",
              " 'catch': tensor(-0.0718),\n",
              " 'cats': tensor(-0.0347),\n",
              " 'caught': tensor(0.0568),\n",
              " 'cauldron': tensor(-0.2125),\n",
              " 'cause': tensor(0.0355),\n",
              " 'ceiling': tensor(0.0196),\n",
              " 'centaur': tensor(-0.0736),\n",
              " 'certainly': tensor(-0.1322),\n",
              " 'chair': tensor(0.0015),\n",
              " 'chamber': tensor(-0.1071),\n",
              " 'chance': tensor(-0.0820),\n",
              " 'change': tensor(-0.0374),\n",
              " 'changed': tensor(-0.0267),\n",
              " 'chapter': tensor(0.0005),\n",
              " 'charlie': tensor(0.0418),\n",
              " 'charlies': tensor(-0.1161),\n",
              " 'charms': tensor(-0.0044),\n",
              " 'chasers': tensor(-0.0074),\n",
              " 'cheer': tensor(0.0184),\n",
              " 'cheering': tensor(0.0955),\n",
              " 'cheers': tensor(-0.0444),\n",
              " 'chess': tensor(0.0672),\n",
              " 'chessmen': tensor(-0.1172),\n",
              " 'chest': tensor(0.1679),\n",
              " 'chocolate': tensor(0.1314),\n",
              " 'christmas': tensor(0.0479),\n",
              " 'chuckled': tensor(-0.0650),\n",
              " 'clambered': tensor(-0.0678),\n",
              " 'clapped': tensor(-0.0546),\n",
              " 'class': tensor(-0.1460),\n",
              " 'classes': tensor(-0.1287),\n",
              " 'classroom': tensor(0.0169),\n",
              " 'clean': tensor(0.0052),\n",
              " 'clear': tensor(0.0114),\n",
              " 'cleared': tensor(0.0230),\n",
              " 'clearing': tensor(-0.0416),\n",
              " 'clearly': tensor(-0.0937),\n",
              " 'clicked': tensor(0.0487),\n",
              " 'climbed': tensor(-0.0012),\n",
              " 'cloak': tensor(-0.0197),\n",
              " 'close': tensor(-0.0009),\n",
              " 'closer': tensor(0.0308),\n",
              " 'clothes': tensor(-0.0445),\n",
              " 'club': tensor(0.1026),\n",
              " 'clutching': tensor(-0.0128),\n",
              " 'coat': tensor(0.0989),\n",
              " 'cold': tensor(0.1567),\n",
              " 'come': tensor(-0.1715),\n",
              " 'coming': tensor(0.0224),\n",
              " 'common': tensor(0.0326),\n",
              " 'compartment': tensor(-0.1002),\n",
              " 'completely': tensor(0.0866),\n",
              " 'computer': tensor(0.0158),\n",
              " 'control': tensor(-0.0703),\n",
              " 'corner': tensor(-0.0063),\n",
              " 'corridor': tensor(-0.0668),\n",
              " 'corridors': tensor(0.0012),\n",
              " 'could': tensor(-0.0442),\n",
              " 'couldnt': tensor(-0.0802),\n",
              " 'couple': tensor(0.0871),\n",
              " 'courage': tensor(0.0241),\n",
              " 'course': tensor(0.0414),\n",
              " 'covered': tensor(0.0444),\n",
              " 'crabbe': tensor(-0.1247),\n",
              " 'crack': tensor(-0.0148),\n",
              " 'crash': tensor(0.1153),\n",
              " 'crate': tensor(-0.0832),\n",
              " 'crept': tensor(0.1164),\n",
              " 'cried': tensor(0.0281),\n",
              " 'cross': tensor(-0.0287),\n",
              " 'crossed': tensor(-0.1768),\n",
              " 'crowd': tensor(0.1073),\n",
              " 'cry': tensor(0.0529),\n",
              " 'crying': tensor(0.1653),\n",
              " 'cup': tensor(-0.0284),\n",
              " 'cupboard': tensor(-0.1117),\n",
              " 'curious': tensor(-0.1595),\n",
              " 'curse': tensor(0.1307),\n",
              " 'cut': tensor(-0.1220),\n",
              " 'dad': tensor(0.0881),\n",
              " 'damp': tensor(-0.0004),\n",
              " 'dangerous': tensor(0.0786),\n",
              " 'dare': tensor(0.0169),\n",
              " 'dark': tensor(-0.0096),\n",
              " 'darkly': tensor(-0.0555),\n",
              " 'darkness': tensor(-0.0740),\n",
              " 'day': tensor(-0.0393),\n",
              " 'days': tensor(0.1159),\n",
              " 'dead': tensor(-0.0502),\n",
              " 'dean': tensor(0.0574),\n",
              " 'dear': tensor(-0.1255),\n",
              " 'death': tensor(-0.0115),\n",
              " 'decided': tensor(-0.0542),\n",
              " 'deep': tensor(-0.0313),\n",
              " 'delighted': tensor(-0.0806),\n",
              " 'desk': tensor(-0.0152),\n",
              " 'desperate': tensor(0.0509),\n",
              " 'diagon': tensor(-0.0650),\n",
              " 'did': tensor(0.1192),\n",
              " 'didnt': tensor(-0.0293),\n",
              " 'die': tensor(-0.0788),\n",
              " 'died': tensor(-0.1001),\n",
              " 'difference': tensor(-0.2970),\n",
              " 'different': tensor(0.0278),\n",
              " 'difficult': tensor(0.0280),\n",
              " 'dinner': tensor(-0.0187),\n",
              " 'direction': tensor(0.0078),\n",
              " 'disappeared': tensor(-0.0685),\n",
              " 'dived': tensor(-0.1752),\n",
              " 'do': tensor(-0.0869),\n",
              " 'does': tensor(0.0525),\n",
              " 'doesnt': tensor(0.0352),\n",
              " 'dog': tensor(-0.0335),\n",
              " 'dogs': tensor(0.0144),\n",
              " 'doing': tensor(0.0540),\n",
              " 'don': tensor(0.0202),\n",
              " 'done': tensor(-0.1078),\n",
              " 'dont': tensor(-0.0167),\n",
              " 'door': tensor(-0.0560),\n",
              " 'doors': tensor(-0.1142),\n",
              " 'doorway': tensor(-0.0972),\n",
              " 'dormitory': tensor(0.0924),\n",
              " 'down': tensor(-0.0644),\n",
              " 'draco': tensor(0.1063),\n",
              " 'dragged': tensor(0.0518),\n",
              " 'dragon': tensor(-0.0013),\n",
              " 'dragons': tensor(0.0404),\n",
              " 'dream': tensor(0.0499),\n",
              " 'dressed': tensor(-0.0235),\n",
              " 'drew': tensor(-0.1277),\n",
              " 'drills': tensor(-0.0683),\n",
              " 'drink': tensor(0.0279),\n",
              " 'drive': tensor(-0.0592),\n",
              " 'drop': tensor(-0.1032),\n",
              " 'dropped': tensor(0.0073),\n",
              " 'drove': tensor(-0.0152),\n",
              " 'dudley': tensor(0.0350),\n",
              " 'dudleys': tensor(0.1375),\n",
              " 'dumbledore': tensor(-0.1082),\n",
              " 'dumbledores': tensor(0.1800),\n",
              " 'dungeons': tensor(0.0586),\n",
              " 'dunno': tensor(0.0634),\n",
              " 'during': tensor(-0.1809),\n",
              " 'dursley': tensor(0.1491),\n",
              " 'dursleys': tensor(0.0874),\n",
              " 'each': tensor(-0.0133),\n",
              " 'eagerly': tensor(0.0366),\n",
              " 'ear': tensor(-0.0745),\n",
              " 'ears': tensor(-0.0098),\n",
              " 'earth': tensor(-0.1088),\n",
              " 'easily': tensor(0.1639),\n",
              " 'easy': tensor(0.1903),\n",
              " 'eat': tensor(-0.1353),\n",
              " 'eating': tensor(0.0253),\n",
              " 'edge': tensor(-0.1369),\n",
              " 'egg': tensor(-0.0166),\n",
              " 'eh': tensor(-0.0323),\n",
              " 'either': tensor(0.0829),\n",
              " 'eleven': tensor(0.1365),\n",
              " 'else': tensor(0.2068),\n",
              " 'em': tensor(-0.0423),\n",
              " 'empty': tensor(0.0495),\n",
              " 'end': tensor(-0.0567),\n",
              " 'enough': tensor(0.1514),\n",
              " 'entered': tensor(-0.0150),\n",
              " 'entrance': tensor(0.0486),\n",
              " 'envelope': tensor(0.0907),\n",
              " 'er': tensor(-0.0955),\n",
              " 'erised': tensor(-0.0575),\n",
              " 'even': tensor(0.0036),\n",
              " 'evening': tensor(-0.0340),\n",
              " 'ever': tensor(0.0760),\n",
              " 'every': tensor(0.0511),\n",
              " 'everybody': tensor(0.0292),\n",
              " 'everyone': tensor(-0.1186),\n",
              " 'everything': tensor(0.0292),\n",
              " 'everywhere': tensor(0.0694),\n",
              " 'evil': tensor(0.0200),\n",
              " 'exactly': tensor(0.0579),\n",
              " 'exam': tensor(0.0023),\n",
              " 'exams': tensor(0.0059),\n",
              " 'excellent': tensor(-0.0258),\n",
              " 'except': tensor(0.1000),\n",
              " 'excitedly': tensor(-0.0100),\n",
              " 'excuse': tensor(0.1084),\n",
              " 'expect': tensor(0.0656),\n",
              " 'expected': tensor(-0.0057),\n",
              " 'expelled': tensor(-0.0874),\n",
              " 'explain': tensor(0.1395),\n",
              " 'extra': tensor(0.1292),\n",
              " 'eye': tensor(-0.0547),\n",
              " 'eyes': tensor(0.0285),\n",
              " 'face': tensor(-0.0124),\n",
              " 'faces': tensor(-0.1185),\n",
              " 'facing': tensor(-0.0979),\n",
              " 'fact': tensor(-0.0318),\n",
              " 'faded': tensor(0.0278),\n",
              " 'fall': tensor(-0.0520),\n",
              " 'fallen': tensor(-0.0140),\n",
              " 'families': tensor(-0.0682),\n",
              " 'family': tensor(-0.0123),\n",
              " 'famous': tensor(-0.1091),\n",
              " 'fang': tensor(-0.0602),\n",
              " 'fangs': tensor(-0.0822),\n",
              " 'far': tensor(-0.0708),\n",
              " 'fast': tensor(0.0383),\n",
              " 'fat': tensor(-0.0409),\n",
              " 'father': tensor(0.0180),\n",
              " 'fathers': tensor(-0.1015),\n",
              " 'favorite': tensor(0.0479),\n",
              " 'fear': tensor(0.0580),\n",
              " 'feast': tensor(0.0508),\n",
              " 'feather': tensor(-0.0181),\n",
              " 'feel': tensor(-0.0821),\n",
              " 'feeling': tensor(0.0687),\n",
              " 'feet': tensor(-0.1655),\n",
              " 'fell': tensor(-0.0004),\n",
              " 'felt': tensor(-0.1603),\n",
              " 'fer': tensor(0.1052),\n",
              " 'few': tensor(-0.1845),\n",
              " 'field': tensor(-0.0370),\n",
              " 'fifty': tensor(0.0660),\n",
              " 'fight': tensor(0.1378),\n",
              " 'fighting': tensor(0.0006),\n",
              " 'figure': tensor(0.0387),\n",
              " 'filch': tensor(-0.0085),\n",
              " 'filled': tensor(0.0499),\n",
              " 'finally': tensor(-0.0419),\n",
              " 'find': tensor(-0.1040),\n",
              " 'finding': tensor(-0.0219),\n",
              " 'fine': tensor(0.0846),\n",
              " 'fingers': tensor(-0.0891),\n",
              " 'finished': tensor(0.0807),\n",
              " 'finnigan': tensor(-0.0140),\n",
              " 'fire': tensor(0.1887),\n",
              " 'firenze': tensor(-0.0824),\n",
              " 'firs': tensor(-0.0332),\n",
              " 'first': tensor(-0.0260),\n",
              " 'five': tensor(-0.0215),\n",
              " 'fixed': tensor(-0.1723),\n",
              " 'flamel': tensor(0.0438),\n",
              " 'flames': tensor(-0.0762),\n",
              " 'flash': tensor(0.0431),\n",
              " 'flat': tensor(-0.0798),\n",
              " 'flavor': tensor(0.0693),\n",
              " 'flew': tensor(-0.2132),\n",
              " 'flint': tensor(-0.2071),\n",
              " 'flitwick': tensor(-0.0897),\n",
              " 'floating': tensor(0.0884),\n",
              " 'floor': tensor(0.0659),\n",
              " 'fluffy': tensor(-0.0681),\n",
              " 'flute': tensor(-0.0433),\n",
              " 'fly': tensor(-0.0454),\n",
              " 'flying': tensor(0.0359),\n",
              " 'follow': tensor(0.0739),\n",
              " 'followed': tensor(0.1449),\n",
              " 'following': tensor(0.1397),\n",
              " 'food': tensor(-0.1152),\n",
              " 'foot': tensor(0.0548),\n",
              " 'footsteps': tensor(-0.1360),\n",
              " 'for': tensor(0.2014),\n",
              " 'forbidden': tensor(0.0298),\n",
              " 'force': tensor(-0.0691),\n",
              " 'forehead': tensor(0.0345),\n",
              " 'forest': tensor(-0.0766),\n",
              " 'forget': tensor(-0.0854),\n",
              " 'forgotten': tensor(0.0121),\n",
              " 'forward': tensor(-0.0976),\n",
              " 'found': tensor(0.2231),\n",
              " 'four': tensor(0.0386),\n",
              " 'fred': tensor(-0.1045),\n",
              " 'free': tensor(0.0380),\n",
              " 'friend': tensor(-0.1650),\n",
              " 'friends': tensor(0.0226),\n",
              " 'frog': tensor(0.1632),\n",
              " 'frogs': tensor(0.0219),\n",
              " 'from': tensor(0.0380),\n",
              " 'front': tensor(-0.2056),\n",
              " 'full': tensor(-0.1628),\n",
              " 'fun': tensor(0.0380),\n",
              " 'funny': tensor(-0.0422),\n",
              " 'furious': tensor(-0.1872),\n",
              " 'furiously': tensor(-0.1101),\n",
              " 'game': tensor(-0.1034),\n",
              " 'garden': tensor(-0.0898),\n",
              " 'gasped': tensor(0.1689),\n",
              " 'gave': tensor(0.0197),\n",
              " 'gently': tensor(0.0475),\n",
              " 'george': tensor(-0.0781),\n",
              " 'get': tensor(0.2482),\n",
              " 'gets': tensor(0.1571),\n",
              " 'gettin': tensor(0.0627),\n",
              " 'getting': tensor(0.0567),\n",
              " 'ghost': tensor(-0.0370),\n",
              " 'ghosts': tensor(-0.0379),\n",
              " 'giant': tensor(0.0193),\n",
              " 'girl': tensor(0.0859),\n",
              " 'girls': tensor(0.0547),\n",
              " 'give': tensor(-0.1738),\n",
              " 'given': tensor(0.1025),\n",
              " 'giving': tensor(0.1074),\n",
              " 'glad': tensor(-0.1601),\n",
              " 'glass': tensor(0.0751),\n",
              " 'glasses': tensor(-0.0826),\n",
              " 'go': tensor(0.0635),\n",
              " 'goal': tensor(0.1082),\n",
              " 'goblin': tensor(0.0371),\n",
              " 'goblins': tensor(0.1467),\n",
              " 'goes': tensor(-0.0188),\n",
              " 'going': tensor(-0.1576),\n",
              " 'gold': tensor(-0.1728),\n",
              " 'golden': tensor(0.0154),\n",
              " 'gone': tensor(-0.0311),\n",
              " 'good': tensor(-0.0701),\n",
              " 'goodbye': tensor(-0.0054),\n",
              " 'got': tensor(-0.0130),\n",
              " 'gotta': tensor(0.0434),\n",
              " 'gotten': tensor(0.1580),\n",
              " 'goyle': tensor(0.1333),\n",
              " 'grab': tensor(-0.1187),\n",
              " 'grabbed': tensor(-0.0264),\n",
              " 'granger': tensor(0.0789),\n",
              " 'grass': tensor(-0.0643),\n",
              " 'gray': tensor(-0.0157),\n",
              " 'great': tensor(-0.0683),\n",
              " 'green': tensor(-0.0862),\n",
              " 'grin': tensor(-0.0018),\n",
              " 'gringotts': tensor(0.0238),\n",
              " 'griphook': tensor(-0.0318),\n",
              " 'ground': tensor(0.0596),\n",
              " 'grounds': tensor(-0.1288),\n",
              " 'growled': tensor(-0.0034),\n",
              " 'grunted': tensor(-0.1105),\n",
              " 'gryffindor': tensor(-0.1505),\n",
              " 'gryffindors': tensor(-0.0041),\n",
              " 'guard': tensor(0.0444),\n",
              " 'guarding': tensor(-0.0116),\n",
              " 'h': tensor(0.0259),\n",
              " 'had': tensor(0.0579),\n",
              " 'hadnt': tensor(0.1303),\n",
              " 'hagrid': tensor(-0.0990),\n",
              " 'hagrids': tensor(0.0150),\n",
              " 'hair': tensor(-0.0336),\n",
              " 'half': tensor(0.0377),\n",
              " 'halfway': tensor(-0.0684),\n",
              " 'hall': tensor(-0.0772),\n",
              " 'halloween': tensor(-0.1714),\n",
              " 'hand': tensor(0.1701),\n",
              " 'handed': tensor(0.0269),\n",
              " 'handle': tensor(-0.1194),\n",
              " 'hands': tensor(0.1484),\n",
              " 'hang': tensor(-0.0050),\n",
              " 'hanging': tensor(0.1205),\n",
              " 'happen': tensor(0.0430),\n",
              " 'happened': tensor(0.0657),\n",
              " 'happy': tensor(0.0913),\n",
              " 'hard': tensor(-0.0834),\n",
              " 'harder': tensor(0.0021),\n",
              " 'hardly': tensor(0.0421),\n",
              " 'harry': tensor(0.8126),\n",
              " 'harrys': tensor(-0.2045),\n",
              " 'has': tensor(0.0534),\n",
              " 'hasnt': tensor(0.0049),\n",
              " 'hat': tensor(-0.1140),\n",
              " 'hate': tensor(0.0551),\n",
              " 'hated': tensor(0.0617),\n",
              " 'have': tensor(0.0181),\n",
              " 'havent': tensor(0.0361),\n",
              " 'having': tensor(-0.0412),\n",
              " 'he': tensor(0.0261),\n",
              " 'head': tensor(-0.0695),\n",
              " 'headless': tensor(-0.0284),\n",
              " 'heads': tensor(0.1104),\n",
              " 'hear': tensor(0.0390),\n",
              " 'heard': tensor(0.1082),\n",
              " 'heart': tensor(-0.0477),\n",
              " 'heavy': tensor(-0.0322),\n",
              " 'hed': tensor(0.0818),\n",
              " 'hedwig': tensor(0.0003),\n",
              " 'held': tensor(-0.0134),\n",
              " 'hell': tensor(-0.0715),\n",
              " 'help': tensor(0.1465),\n",
              " 'her': tensor(-0.0412),\n",
              " 'here': tensor(-0.1425),\n",
              " 'hermione': tensor(0.2639),\n",
              " 'hermiones': tensor(0.0132),\n",
              " 'herself': tensor(-0.1026),\n",
              " 'hes': tensor(-0.1743),\n",
              " 'hidden': tensor(0.0691),\n",
              " 'hide': tensor(-0.0585),\n",
              " 'hiding': tensor(0.0748),\n",
              " 'high': tensor(-0.0025),\n",
              " 'higher': tensor(0.0546),\n",
              " 'him': tensor(-0.0428),\n",
              " 'himself': tensor(0.0646),\n",
              " 'his': tensor(-0.0748),\n",
              " 'hissed': tensor(0.0858),\n",
              " 'history': tensor(0.0301),\n",
              " 'hit': tensor(0.1483),\n",
              " 'hogwarts': tensor(-0.0546),\n",
              " 'hold': tensor(0.0079),\n",
              " 'holding': tensor(-0.0151),\n",
              " 'hole': tensor(-0.0053),\n",
              " 'holidays': tensor(0.0500),\n",
              " 'home': tensor(-0.1442),\n",
              " 'homework': tensor(-0.0318),\n",
              " 'honestly': tensor(0.0467),\n",
              " 'hooch': tensor(0.1732),\n",
              " 'hoops': tensor(0.0268),\n",
              " 'hope': tensor(0.0094),\n",
              " 'hoping': tensor(0.0757),\n",
              " 'horrible': tensor(0.0131),\n",
              " 'horror': tensor(0.2240),\n",
              " 'hospital': tensor(0.0821),\n",
              " 'hot': tensor(-0.1058),\n",
              " 'hour': tensor(0.0020),\n",
              " 'hours': tensor(-0.1054),\n",
              " 'house': tensor(0.0267),\n",
              " 'houses': tensor(-0.1495),\n",
              " 'how': tensor(0.0842),\n",
              " 'however': tensor(0.0015),\n",
              " 'howling': tensor(0.1181),\n",
              " 'hufflepuff': tensor(-0.0716),\n",
              " 'huge': tensor(0.1186),\n",
              " 'human': tensor(-0.1003),\n",
              " 'hundred': tensor(-0.0424),\n",
              " 'hundreds': tensor(0.0373),\n",
              " 'hung': tensor(-0.0224),\n",
              " 'hungry': tensor(0.0222),\n",
              " 'hurried': tensor(-0.0924),\n",
              " 'hurry': tensor(0.0979),\n",
              " 'hurrying': tensor(0.0316),\n",
              " 'hurt': tensor(-0.0091),\n",
              " 'hut': tensor(0.0758),\n",
              " 'i': tensor(-0.0357),\n",
              " 'ice': tensor(-0.1630),\n",
              " 'id': tensor(0.0083),\n",
              " 'idea': tensor(-0.2307),\n",
              " 'if': tensor(-0.0305),\n",
              " 'ignored': tensor(0.0525),\n",
              " 'ill': tensor(0.0678),\n",
              " 'im': tensor(-0.0519),\n",
              " 'imagine': tensor(-0.0508),\n",
              " 'important': tensor(0.1370),\n",
              " 'in': tensor(-0.0057),\n",
              " 'inches': tensor(0.0828),\n",
              " 'indeed': tensor(0.0216),\n",
              " 'inside': tensor(-0.0725),\n",
              " 'instead': tensor(0.0348),\n",
              " 'interested': tensor(0.0581),\n",
              " 'interesting': tensor(-0.0352),\n",
              " 'into': tensor(0.0221),\n",
              " 'invisibility': tensor(0.0831),\n",
              " 'invisible': tensor(-0.0181),\n",
              " 'is': tensor(0.2109),\n",
              " 'isnt': tensor(-0.1213),\n",
              " 'it': tensor(0.0129),\n",
              " 'itll': tensor(-0.0477),\n",
              " 'its': tensor(0.0737),\n",
              " 'itself': tensor(0.0842),\n",
              " 'ive': tensor(0.1491),\n",
              " 'jerked': tensor(0.0988),\n",
              " 'job': tensor(0.1605),\n",
              " 'join': tensor(0.0521),\n",
              " 'joined': tensor(0.0856),\n",
              " 'joke': tensor(-0.0230),\n",
              " 'jordan': tensor(0.0657),\n",
              " 'jump': tensor(-0.1057),\n",
              " 'jumped': tensor(-0.1195),\n",
              " 'jus': tensor(-0.2193),\n",
              " 'just': tensor(0.2340),\n",
              " 'keep': tensor(-0.0455),\n",
              " 'keeper': tensor(0.0609),\n",
              " 'keeping': tensor(-0.1240),\n",
              " 'kept': tensor(0.0191),\n",
              " 'key': tensor(0.1306),\n",
              " 'keys': tensor(0.0634),\n",
              " 'kicked': tensor(0.0017),\n",
              " 'kill': tensor(0.0432),\n",
              " 'killed': tensor(0.0234),\n",
              " 'kind': tensor(0.0432),\n",
              " 'kings': tensor(-0.0522),\n",
              " 'kitchen': tensor(-0.0997),\n",
              " 'knees': tensor(0.0815),\n",
              " 'knew': tensor(-0.0979),\n",
              " 'knight': tensor(0.1120),\n",
              " 'knock': tensor(0.1008),\n",
              " 'knocked': tensor(0.0632),\n",
              " 'knocking': tensor(0.0287),\n",
              " 'know': tensor(0.0190),\n",
              " 'knowing': tensor(0.1178),\n",
              " 'known': tensor(-0.0565),\n",
              " 'knows': tensor(0.1883),\n",
              " 'knuts': tensor(0.1626),\n",
              " 'lady': tensor(0.0181),\n",
              " 'lake': tensor(-0.0470),\n",
              " 'lamp': tensor(0.1087),\n",
              " 'landed': tensor(0.0104),\n",
              " 'large': tensor(-0.1151),\n",
              " 'last': tensor(-0.0801),\n",
              " 'late': tensor(0.0142),\n",
              " 'later': tensor(0.0173),\n",
              " 'laugh': tensor(0.0109),\n",
              " 'laughed': tensor(-0.0012),\n",
              " 'laughing': tensor(0.0813),\n",
              " 'laughter': tensor(0.0274),\n",
              " 'lay': tensor(0.0659),\n",
              " 'lead': tensor(-0.0044),\n",
              " 'leading': tensor(0.0785),\n",
              " 'leaky': tensor(-0.0498),\n",
              " 'leaned': tensor(0.1816),\n",
              " 'leapt': tensor(0.1135),\n",
              " 'learn': tensor(0.0682),\n",
              " 'learned': tensor(-0.0717),\n",
              " 'least': tensor(-0.0984),\n",
              " 'leave': tensor(-0.1525),\n",
              " 'leaves': tensor(-0.0872),\n",
              " 'leaving': tensor(-0.1207),\n",
              " 'led': tensor(-0.0506),\n",
              " 'lee': tensor(-0.0372),\n",
              " 'left': tensor(0.0820),\n",
              " 'leg': tensor(-0.1801),\n",
              " 'legs': tensor(0.0278),\n",
              " 'lemon': tensor(0.0594),\n",
              " 'less': tensor(0.1477),\n",
              " 'lesson': tensor(-0.0800),\n",
              " 'lessons': tensor(0.0197),\n",
              " 'let': tensor(0.1314),\n",
              " 'lets': tensor(-0.0110),\n",
              " 'letter': tensor(-0.0474),\n",
              " 'letters': tensor(0.0467),\n",
              " 'library': tensor(0.0134),\n",
              " 'lie': tensor(-0.0179),\n",
              " 'life': tensor(0.1885),\n",
              " 'light': tensor(-0.0771),\n",
              " 'lightning': tensor(-0.0813),\n",
              " 'like': tensor(0.0208),\n",
              " 'liked': tensor(0.0194),\n",
              " 'lily': tensor(-0.0621),\n",
              " 'line': tensor(0.0058),\n",
              " 'lips': tensor(0.0741),\n",
              " 'list': tensor(0.0739),\n",
              " 'listen': tensor(0.0187),\n",
              " 'listening': tensor(-0.1281),\n",
              " 'lit': tensor(-0.0797),\n",
              " 'little': tensor(-0.2419),\n",
              " 'live': tensor(0.2508),\n",
              " 'lived': tensor(0.0362),\n",
              " 'living': tensor(-0.1060),\n",
              " 'loads': tensor(-0.0467),\n",
              " 'lock': tensor(-0.0553),\n",
              " 'locked': tensor(-0.0157),\n",
              " 'london': tensor(-0.0537),\n",
              " 'long': tensor(0.0372),\n",
              " 'longbottom': tensor(-0.0391),\n",
              " 'look': tensor(0.0027),\n",
              " 'looked': tensor(-0.0112),\n",
              " 'looking': tensor(-0.1459),\n",
              " 'looks': tensor(0.0245),\n",
              " 'lose': tensor(-0.1137),\n",
              " 'losing': tensor(0.0770),\n",
              " 'lost': tensor(0.1390),\n",
              " 'lot': tensor(-0.1227),\n",
              " 'lots': tensor(0.0862),\n",
              " 'loud': tensor(0.1653),\n",
              " 'loudly': tensor(-0.0036),\n",
              " 'low': tensor(-0.0346),\n",
              " 'luck': tensor(-0.1783),\n",
              " 'lucky': tensor(-0.1026),\n",
              " 'lumpy': tensor(0.0640),\n",
              " 'lurking': tensor(-0.0179),\n",
              " 'lying': tensor(0.0286),\n",
              " 'mad': tensor(0.0198),\n",
              " 'madam': tensor(0.0926),\n",
              " 'made': tensor(-0.0371),\n",
              " 'magic': tensor(0.0726),\n",
              " 'magical': tensor(-0.0155),\n",
              " 'mail': tensor(0.0322),\n",
              " 'make': tensor(0.0854),\n",
              " 'making': tensor(0.1176),\n",
              " 'malfoy': tensor(0.0744),\n",
              " 'malfoys': tensor(0.1380),\n",
              " 'man': tensor(0.0348),\n",
              " 'managed': tensor(-0.0937),\n",
              " 'many': tensor(-0.0128),\n",
              " 'marble': tensor(-0.0965),\n",
              " 'marched': tensor(-0.0256),\n",
              " 'match': tensor(0.1006),\n",
              " 'matter': tensor(0.0637),\n",
              " 'may': tensor(0.1092),\n",
              " 'maybe': tensor(-0.1332),\n",
              " 'mcgonagall': tensor(-0.0226),\n",
              " 'mcgonagalls': tensor(-0.0851),\n",
              " 'me': tensor(0.0818),\n",
              " 'mean': tensor(0.0358),\n",
              " 'means': tensor(0.2045),\n",
              " 'meant': tensor(-0.0079),\n",
              " 'meet': tensor(0.0466),\n",
              " 'mention': tensor(0.0635),\n",
              " 'met': tensor(-0.0130),\n",
              " 'midair': tensor(0.0063),\n",
              " 'middle': tensor(0.0089),\n",
              " 'midnight': tensor(-0.0529),\n",
              " 'might': tensor(0.0143),\n",
              " 'mind': tensor(-0.0693),\n",
              " 'ministry': tensor(-0.1710),\n",
              " 'minute': tensor(-0.0001),\n",
              " 'minutes': tensor(0.2102),\n",
              " 'mirror': tensor(-0.0102),\n",
              " 'miss': tensor(-0.3001),\n",
              " 'mistake': tensor(-0.0982),\n",
              " 'moaned': tensor(0.0535),\n",
              " 'mom': tensor(0.0809),\n",
              " 'moment': tensor(0.0264),\n",
              " 'money': tensor(0.0427),\n",
              " 'moonlight': tensor(-0.2003),\n",
              " 'more': tensor(-0.0765),\n",
              " 'morning': tensor(0.0288),\n",
              " 'most': tensor(0.1425),\n",
              " 'mother': tensor(-0.0557),\n",
              " 'mothers': tensor(-0.0834),\n",
              " 'motorcycle': tensor(0.0071),\n",
              " 'mountain': tensor(0.0948),\n",
              " 'mouth': tensor(-0.0095),\n",
              " 'move': tensor(-0.0111),\n",
              " 'moved': tensor(0.0434),\n",
              " 'moving': tensor(-0.0017),\n",
              " 'mr': tensor(-0.0363),\n",
              " 'mrs': tensor(-0.0184),\n",
              " 'much': tensor(0.0652),\n",
              " 'muggle': tensor(0.0603),\n",
              " 'muggles': tensor(0.1079),\n",
              " 'murmured': tensor(-0.1256),\n",
              " 'must': tensor(-0.0714),\n",
              " 'mustache': tensor(0.1201),\n",
              " 'mustve': tensor(0.0854),\n",
              " 'muttered': tensor(-0.0075),\n",
              " 'muttering': tensor(0.0496),\n",
              " 'my': tensor(0.0278),\n",
              " 'myself': tensor(-0.0692),\n",
              " 'mysterious': tensor(0.2196),\n",
              " 'nah': tensor(0.0615),\n",
              " 'name': tensor(-0.0203),\n",
              " 'names': tensor(0.0580),\n",
              " 'narrow': tensor(0.0603),\n",
              " 'nasty': tensor(-0.2320),\n",
              " 'near': tensor(-0.0277),\n",
              " 'nearer': tensor(0.0628),\n",
              " 'nearest': tensor(0.0688),\n",
              " 'nearly': tensor(-0.0827),\n",
              " 'neck': tensor(0.0541),\n",
              " 'need': tensor(-0.1094),\n",
              " 'needed': tensor(0.0529),\n",
              " 'needs': tensor(-0.0642),\n",
              " 'neither': tensor(-0.0355),\n",
              " 'nervous': tensor(0.1441),\n",
              " 'nervously': tensor(-0.1468),\n",
              " 'never': tensor(0.0729),\n",
              " 'neville': tensor(-0.0323),\n",
              " 'nevilles': tensor(-0.0730),\n",
              " 'new': tensor(-0.0021),\n",
              " 'news': tensor(-0.1266),\n",
              " 'newspaper': tensor(0.1057),\n",
              " 'next': tensor(0.0469),\n",
              " 'nice': tensor(-0.0010),\n",
              " 'nicolas': tensor(0.0016),\n",
              " 'night': tensor(-0.0344),\n",
              " 'nimbus': tensor(0.0375),\n",
              " 'nine': tensor(0.0203),\n",
              " 'no': tensor(0.0627),\n",
              " 'nobody': tensor(-0.0851),\n",
              " 'nodded': tensor(-0.0353),\n",
              " 'noise': tensor(0.0938),\n",
              " 'none': tensor(0.0083),\n",
              " 'nor': tensor(-0.0716),\n",
              " 'norbert': tensor(-0.0025),\n",
              " 'normal': tensor(-0.0895),\n",
              " 'norris': tensor(0.1910),\n",
              " 'nose': tensor(0.0898),\n",
              " 'noses': tensor(-0.1250),\n",
              " 'nostrils': tensor(-0.0856),\n",
              " 'not': tensor(0.0648),\n",
              " 'note': tensor(0.0809),\n",
              " 'notes': tensor(0.1207),\n",
              " 'nothin': tensor(-0.2128),\n",
              " 'nothing': tensor(0.0251),\n",
              " 'notice': tensor(-0.1293),\n",
              " 'noticed': tensor(-0.1323),\n",
              " 'noticing': tensor(0.1155),\n",
              " 'now': tensor(-0.1029),\n",
              " 'number': tensor(-0.0932),\n",
              " 'o': tensor(-0.0590),\n",
              " 'obviously': tensor(-0.0652),\n",
              " 'oclock': tensor(-0.0627),\n",
              " 'odd': tensor(0.0342),\n",
              " 'of': tensor(-0.0390),\n",
              " 'off': tensor(0.0276),\n",
              " 'often': tensor(0.0252),\n",
              " 'oh': tensor(0.0600),\n",
              " 'old': tensor(-0.0280),\n",
              " 'older': tensor(-0.1393),\n",
              " 'ollivander': tensor(0.0067),\n",
              " 'on': tensor(-0.0117),\n",
              " 'once': tensor(0.0584),\n",
              " 'one': tensor(0.1429),\n",
              " 'ones': tensor(-0.0501),\n",
              " 'only': tensor(0.0082),\n",
              " 'onto': tensor(0.1215),\n",
              " 'open': tensor(-0.0071),\n",
              " 'opened': tensor(0.0109),\n",
              " 'opposite': tensor(-0.0438),\n",
              " 'or': tensor(0.0508),\n",
              " 'ordinary': tensor(-0.1198),\n",
              " 'other': tensor(-0.0606),\n",
              " 'others': tensor(-0.0986),\n",
              " 'our': tensor(0.1086),\n",
              " 'out': tensor(-0.0188),\n",
              " 'outside': tensor(-0.0431),\n",
              " 'outta': tensor(-0.0778),\n",
              " 'over': tensor(-0.0417),\n",
              " 'overhead': tensor(0.0979),\n",
              " 'owl': tensor(0.1289),\n",
              " 'owls': tensor(0.1070),\n",
              " 'own': tensor(0.1047),\n",
              " 'pack': tensor(-0.1561),\n",
              " 'package': tensor(0.0929),\n",
              " 'packed': tensor(-0.0278),\n",
              " 'pain': tensor(0.0488),\n",
              " 'pair': tensor(0.0352),\n",
              " 'pale': tensor(-0.0952),\n",
              " 'panted': tensor(-0.0055),\n",
              " 'paper': tensor(-0.1751),\n",
              " 'parcel': tensor(0.0156),\n",
              " 'parchment': tensor(0.0103),\n",
              " 'parents': tensor(-0.2336),\n",
              " 'particularly': tensor(-0.0510),\n",
              " 'passageway': tensor(-0.1845),\n",
              " 'passed': tensor(-0.0566),\n",
              " 'passing': tensor(-0.0834),\n",
              " 'past': tensor(0.1782),\n",
              " 'path': tensor(0.0385),\n",
              " 'patil': tensor(0.0213),\n",
              " 'peered': tensor(0.1306),\n",
              " 'peering': tensor(0.0185),\n",
              " 'peeves': tensor(-0.0786),\n",
              " 'people': tensor(0.1291),\n",
              " 'percy': tensor(-0.0076),\n",
              " 'perfect': tensor(0.1362),\n",
              " 'perhaps': tensor(0.0118),\n",
              " 'person': tensor(0.1424),\n",
              " 'petunia': tensor(0.0590),\n",
              " 'picked': tensor(-0.0078),\n",
              " 'piece': tensor(-0.0043),\n",
              " 'pieces': tensor(0.0069),\n",
              " 'piers': tensor(-0.3048),\n",
              " 'pig': tensor(0.1446),\n",
              " 'pile': tensor(0.0625),\n",
              " 'piled': tensor(-0.0395),\n",
              " 'pink': tensor(0.0074),\n",
              " 'pinned': tensor(-0.0240),\n",
              " 'place': tensor(-0.1167),\n",
              " 'planets': tensor(0.0307),\n",
              " 'plant': tensor(0.0806),\n",
              " 'plates': tensor(0.1745),\n",
              " 'platform': tensor(0.1372),\n",
              " 'platforms': tensor(0.0047),\n",
              " 'play': tensor(0.0295),\n",
              " 'players': tensor(-0.0657),\n",
              " 'playing': tensor(-0.0955),\n",
              " 'please': tensor(0.0534),\n",
              " 'pleased': tensor(-0.2327),\n",
              " 'pocket': tensor(-0.0746),\n",
              " 'pockets': tensor(0.0137),\n",
              " 'point': tensor(-0.0573),\n",
              " 'pointed': tensor(-0.0268),\n",
              " 'pointing': tensor(-0.0271),\n",
              " 'points': tensor(-0.0151),\n",
              " 'pomfrey': tensor(-0.0103),\n",
              " 'poor': tensor(-0.0113),\n",
              " 'portrait': tensor(0.0204),\n",
              " 'possession': tensor(-0.0159),\n",
              " 'possible': tensor(-0.0148),\n",
              " 'posts': tensor(0.0377),\n",
              " 'potion': tensor(0.0646),\n",
              " 'potions': tensor(0.1431),\n",
              " 'potter': tensor(0.0447),\n",
              " 'potters': tensor(-0.1475),\n",
              " 'power': tensor(0.0215),\n",
              " 'powerful': tensor(0.0531),\n",
              " 'practice': tensor(-0.1065),\n",
              " 'prefect': tensor(-0.0218),\n",
              " 'prefects': tensor(-0.0880),\n",
              " 'presents': tensor(-0.0040),\n",
              " 'pressed': tensor(0.0552),\n",
              " 'privet': tensor(-0.0068),\n",
              " 'probably': tensor(-0.0868),\n",
              " 'professor': tensor(-0.0743),\n",
              " 'properly': tensor(-0.1077),\n",
              " 'proud': tensor(0.1651),\n",
              " 'pull': tensor(0.0363),\n",
              " 'pulled': tensor(-0.0777),\n",
              " 'pulling': tensor(-0.0951),\n",
              " 'purple': tensor(0.0858),\n",
              " 'pushed': tensor(-0.0064),\n",
              " 'put': tensor(0.0276),\n",
              " 'quaffle': tensor(-0.0691),\n",
              " 'question': tensor(0.0973),\n",
              " 'questions': tensor(-0.0773),\n",
              " 'quick': tensor(0.2024),\n",
              " 'quickly': tensor(0.0235),\n",
              " 'quidditch': tensor(-0.0083),\n",
              " 'quiet': tensor(0.0683),\n",
              " 'quietly': tensor(0.0216),\n",
              " 'quills': tensor(0.0302),\n",
              " 'quirrell': tensor(-0.0838),\n",
              " 'quirrells': tensor(0.0053),\n",
              " 'quite': tensor(0.0071),\n",
              " 'racing': tensor(0.0297),\n",
              " 'raised': tensor(0.1397),\n",
              " 'ran': tensor(0.0174),\n",
              " 'rang': tensor(0.0659),\n",
              " 'rat': tensor(-0.0735),\n",
              " 'rather': tensor(-0.0191),\n",
              " 'ravenclaw': tensor(0.0267),\n",
              " 'reached': tensor(-0.0607),\n",
              " 'read': tensor(0.0350),\n",
              " 'ready': tensor(-0.0553),\n",
              " 'real': tensor(0.0538),\n",
              " 'realize': tensor(0.0530),\n",
              " 'realized': tensor(0.1178),\n",
              " 'really': tensor(0.1143),\n",
              " 'reason': tensor(-0.1377),\n",
              " ...}"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ],
      "source": [
        "# we can get the dot product value for every other words in the vocab\n",
        "# to get  P(word | harry)\n",
        "word_dot_dict = {}\n",
        "for word in filtered_vocab:\n",
        "  w_idx = word2idx[word]\n",
        "  w_vector = word_vectors[w_idx]\n",
        "  word_dot_dict[word] = sum(w_vector * harry)\n",
        "word_dot_dict"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EXrRV6xoyhDY"
      },
      "source": [
        "Now, let's convert these dot products to probabilities using the softmax function:\n",
        "- We have to convert our prediction into probability distribution to get P(word|harry) so that sum of [P(a|harry), ..., P(potter|harry), ... P(ron|harry), ... ] = 1\n",
        "- current dot product value is any real number, sometimes called as logit\n",
        "  - logit from logistic regression. Some values that are not yet converted to 0-1 or value before sigmoid function\n",
        "  - every probability should be in range (0, 1) (greater than 0, smaller than 1)\n",
        "  - this can be handled by taking exponential of dot product values, divided by total sum\n",
        "  - This function is called **Softmax**\n",
        "\n",
        "- Why we use exponential?\n",
        "  - Because we want to make every probability in positive range while preserving the order\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "id": "DQ1PUvuLyv6r"
      },
      "outputs": [],
      "source": [
        "from math import exp\n",
        "word_exp_dict = {}\n",
        "for word, dot_value in word_dot_dict.items():\n",
        "  exp_value = torch.exp(dot_value)\n",
        "  word_exp_dict[word] = exp_value\n",
        "word_exp_dict\n",
        "word_prob_dict = None"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "id": "SpXH7RxSbwUO"
      },
      "outputs": [],
      "source": [
        "# Get P(potter|harry)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kKxz9SJhSuhy"
      },
      "source": [
        "## 13. Efficient Matrix Operations\n",
        "![img](https://mkang32.github.io/images/python/khan_academy_matrix_product.png)\n",
        "\n",
        "Instead of calculating dot products one by one, we can use matrix multiplication for efficiency:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "HCFajiDkb44W"
      },
      "outputs": [],
      "source": [
        "# get dot product result for every word in the vocabulary\n",
        "\n",
        "# first, make vector_of_harry into matrix format\n",
        "\n",
        "# do matrix multiplication\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KrOLs6kgyhDY"
      },
      "source": [
        "Let's verify that our matrix multiplication gives the same result as individual dot products:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "jf643k38yhDY"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q7DIzDCpyhDY"
      },
      "source": [
        "Now let's implement the complete softmax calculation using matrix operations:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "5USdps5CfKzq"
      },
      "outputs": [],
      "source": [
        "# convert dot product result into exponential"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "WNq0gqz0fntB"
      },
      "outputs": [],
      "source": [
        "# get the sum of exponential\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "ruV4_myDgLvG"
      },
      "outputs": [],
      "source": [
        "# divide exponential value with sum"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nRfw7T5nyhDY"
      },
      "source": [
        "## 14. Creating a Probability Function\n",
        "\n",
        "Let's create a function to calculate probabilities efficiently:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "EF4gutuEgntj"
      },
      "outputs": [],
      "source": [
        "def get_probs(query_vectors, entire_vectors):\n",
        "  return None\n",
        "\n",
        "# get_probs(mat_of_harry, word_vectors)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IOo5uSXbyhDY"
      },
      "source": [
        "## 15. Preparing for Training\n",
        "\n",
        "Before training our Word2Vec model, we need to split our dataset into training and testing sets:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "3gjCSY3DoaFg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "outputId": "58d0676d-0174-4036-a5b7-713c83c187bf"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[(527, 953),\n",
              " (527, 33),\n",
              " (953, 527),\n",
              " (953, 33),\n",
              " (953, 1266),\n",
              " (33, 527),\n",
              " (33, 953),\n",
              " (33, 1266),\n",
              " (33, 1154),\n",
              " (1266, 953),\n",
              " (1266, 33),\n",
              " (1266, 1154),\n",
              " (1266, 1197),\n",
              " (1154, 33),\n",
              " (1154, 1266),\n",
              " (1154, 1197),\n",
              " (1154, 181),\n",
              " (1197, 1266),\n",
              " (1197, 1154),\n",
              " (1197, 181),\n",
              " (1197, 872),\n",
              " (181, 1154),\n",
              " (181, 1197),\n",
              " (181, 872),\n",
              " (181, 1266),\n",
              " (872, 1197),\n",
              " (872, 181),\n",
              " (872, 1266),\n",
              " (872, 123),\n",
              " (1266, 181),\n",
              " (1266, 872),\n",
              " (1266, 123),\n",
              " (1266, 1434),\n",
              " (123, 872),\n",
              " (123, 1266),\n",
              " (123, 1434),\n",
              " (123, 715),\n",
              " (1434, 1266),\n",
              " (1434, 123),\n",
              " (1434, 715),\n",
              " (1434, 795),\n",
              " (715, 123),\n",
              " (715, 1434),\n",
              " (715, 795),\n",
              " (715, 33),\n",
              " (795, 1434),\n",
              " (795, 715),\n",
              " (795, 33),\n",
              " (795, 796),\n",
              " (33, 715),\n",
              " (33, 795),\n",
              " (33, 796),\n",
              " (33, 311),\n",
              " (796, 795),\n",
              " (796, 33),\n",
              " (796, 311),\n",
              " (796, 863),\n",
              " (311, 33),\n",
              " (311, 796),\n",
              " (311, 863),\n",
              " (311, 858),\n",
              " (863, 796),\n",
              " (863, 311),\n",
              " (863, 858),\n",
              " (863, 437),\n",
              " (858, 311),\n",
              " (858, 863),\n",
              " (858, 437),\n",
              " (858, 962),\n",
              " (437, 863),\n",
              " (437, 858),\n",
              " (437, 962),\n",
              " (437, 300),\n",
              " (962, 858),\n",
              " (962, 437),\n",
              " (962, 300),\n",
              " (962, 1418),\n",
              " (300, 437),\n",
              " (300, 962),\n",
              " (300, 1418),\n",
              " (300, 966),\n",
              " (1418, 962),\n",
              " (1418, 300),\n",
              " (1418, 966),\n",
              " (1418, 1307),\n",
              " (966, 300),\n",
              " (966, 1418),\n",
              " (966, 1307),\n",
              " (966, 1042),\n",
              " (1307, 1418),\n",
              " (1307, 966),\n",
              " (1307, 1042),\n",
              " (1307, 1264),\n",
              " (1042, 966),\n",
              " (1042, 1307),\n",
              " (1042, 1264),\n",
              " (1042, 1274),\n",
              " (1264, 1307),\n",
              " (1264, 1042),\n",
              " (1264, 1274),\n",
              " (1264, 1418),\n",
              " (1274, 1042),\n",
              " (1274, 1264),\n",
              " (1274, 1418),\n",
              " (1418, 1264),\n",
              " (1418, 1274),\n",
              " (1418, 844),\n",
              " (844, 1418),\n",
              " (844, 1262),\n",
              " (844, 1494),\n",
              " (1262, 844),\n",
              " (1262, 1494),\n",
              " (1262, 1372),\n",
              " (1494, 844),\n",
              " (1494, 1262),\n",
              " (1494, 1372),\n",
              " (1494, 797),\n",
              " (1372, 1262),\n",
              " (1372, 1494),\n",
              " (1372, 797),\n",
              " (797, 1494),\n",
              " (797, 1372),\n",
              " (1274, 1418),\n",
              " (1274, 1266),\n",
              " (1418, 1274),\n",
              " (1418, 1266),\n",
              " (1418, 667),\n",
              " (1266, 1274),\n",
              " (1266, 1418),\n",
              " (1266, 667),\n",
              " (1266, 913),\n",
              " (667, 1418),\n",
              " (667, 1266),\n",
              " (667, 913),\n",
              " (667, 1495),\n",
              " (913, 1266),\n",
              " (913, 667),\n",
              " (913, 1495),\n",
              " (913, 353),\n",
              " (1495, 667),\n",
              " (1495, 913),\n",
              " (1495, 353),\n",
              " (1495, 1307),\n",
              " (353, 913),\n",
              " (353, 1495),\n",
              " (353, 1307),\n",
              " (353, 75),\n",
              " (1307, 1495),\n",
              " (1307, 353),\n",
              " (1307, 75),\n",
              " (75, 353),\n",
              " (75, 1307),\n",
              " (75, 612),\n",
              " (612, 75),\n",
              " (612, 42),\n",
              " (612, 1206),\n",
              " (42, 612),\n",
              " (42, 1206),\n",
              " (42, 879),\n",
              " (1206, 612),\n",
              " (1206, 42),\n",
              " (1206, 879),\n",
              " (1206, 808),\n",
              " (879, 42),\n",
              " (879, 1206),\n",
              " (879, 808),\n",
              " (879, 79),\n",
              " (808, 1206),\n",
              " (808, 879),\n",
              " (808, 79),\n",
              " (808, 1274),\n",
              " (79, 879),\n",
              " (79, 808),\n",
              " (79, 1274),\n",
              " (79, 638),\n",
              " (1274, 808),\n",
              " (1274, 79),\n",
              " (1274, 638),\n",
              " (1274, 267),\n",
              " (638, 79),\n",
              " (638, 1274),\n",
              " (638, 267),\n",
              " (638, 568),\n",
              " (267, 1274),\n",
              " (267, 638),\n",
              " (267, 568),\n",
              " (267, 1451),\n",
              " (568, 638),\n",
              " (568, 267),\n",
              " (568, 1451),\n",
              " (568, 1218),\n",
              " (1451, 267),\n",
              " (1451, 568),\n",
              " (1451, 1218),\n",
              " (1218, 568),\n",
              " (1218, 1451),\n",
              " (795, 311),\n",
              " (795, 1398),\n",
              " (311, 795),\n",
              " (311, 1398),\n",
              " (311, 1266),\n",
              " (1398, 795),\n",
              " (1398, 311),\n",
              " (1398, 1266),\n",
              " (1266, 311),\n",
              " (1266, 1398),\n",
              " (1266, 863),\n",
              " (863, 1266),\n",
              " (863, 0),\n",
              " (0, 863),\n",
              " (0, 153),\n",
              " (153, 0),\n",
              " (153, 1429),\n",
              " (1429, 153),\n",
              " (1429, 742),\n",
              " (1429, 298),\n",
              " (742, 1429),\n",
              " (742, 298),\n",
              " (298, 1429),\n",
              " (298, 742),\n",
              " (537, 1398),\n",
              " (537, 0),\n",
              " (1398, 537),\n",
              " (1398, 0),\n",
              " (1398, 96),\n",
              " (0, 537),\n",
              " (0, 1398),\n",
              " (0, 96),\n",
              " (96, 1398),\n",
              " (96, 0),\n",
              " (96, 750),\n",
              " (750, 96),\n",
              " (750, 1451),\n",
              " (750, 526),\n",
              " (1451, 750),\n",
              " (1451, 526),\n",
              " (1451, 38),\n",
              " (526, 750),\n",
              " (526, 1451),\n",
              " (526, 38),\n",
              " (526, 818),\n",
              " (38, 1451),\n",
              " (38, 526),\n",
              " (38, 818),\n",
              " (38, 29),\n",
              " (818, 526),\n",
              " (818, 38),\n",
              " (818, 29),\n",
              " (818, 537),\n",
              " (29, 38),\n",
              " (29, 818),\n",
              " (29, 537),\n",
              " (29, 266),\n",
              " (537, 818),\n",
              " (537, 29),\n",
              " (537, 266),\n",
              " (537, 534),\n",
              " (266, 29),\n",
              " (266, 537),\n",
              " (266, 534),\n",
              " (266, 0),\n",
              " (534, 537),\n",
              " (534, 266),\n",
              " (534, 0),\n",
              " (534, 1372),\n",
              " (0, 266),\n",
              " (0, 534),\n",
              " (0, 1372),\n",
              " (0, 666),\n",
              " (1372, 534),\n",
              " (1372, 0),\n",
              " (1372, 666),\n",
              " (1372, 802),\n",
              " (666, 0),\n",
              " (666, 1372),\n",
              " (666, 802),\n",
              " (802, 1372),\n",
              " (802, 666),\n",
              " (796, 311),\n",
              " (796, 1398),\n",
              " (311, 796),\n",
              " (311, 1398),\n",
              " (311, 1280),\n",
              " (1398, 796),\n",
              " (1398, 311),\n",
              " (1398, 1280),\n",
              " (1398, 33),\n",
              " (1280, 311),\n",
              " (1280, 1398),\n",
              " (1280, 33),\n",
              " (33, 1398),\n",
              " (33, 1280),\n",
              " (33, 33),\n",
              " (33, 33),\n",
              " (33, 506),\n",
              " (33, 817),\n",
              " (506, 33),\n",
              " (506, 817),\n",
              " (506, 1345),\n",
              " (817, 33),\n",
              " (817, 506),\n",
              " (817, 1345),\n",
              " (817, 1266),\n",
              " (1345, 506),\n",
              " (1345, 817),\n",
              " (1345, 1266),\n",
              " (1345, 1366),\n",
              " (1266, 817),\n",
              " (1266, 1345),\n",
              " (1266, 1366),\n",
              " (1366, 1345),\n",
              " (1366, 1266),\n",
              " (1366, 863),\n",
              " (863, 1366),\n",
              " (863, 818),\n",
              " (863, 1429),\n",
              " (818, 863),\n",
              " (818, 1429),\n",
              " (818, 154),\n",
              " (1429, 863),\n",
              " (1429, 818),\n",
              " (1429, 154),\n",
              " (1429, 612),\n",
              " (154, 818),\n",
              " (154, 1429),\n",
              " (154, 612),\n",
              " (154, 1372),\n",
              " (612, 1429),\n",
              " (612, 154),\n",
              " (612, 1372),\n",
              " (612, 1365),\n",
              " (1372, 154),\n",
              " (1372, 612),\n",
              " (1372, 1365),\n",
              " (1372, 55),\n",
              " (1365, 612),\n",
              " (1365, 1372),\n",
              " (1365, 55),\n",
              " (1365, 1077),\n",
              " (55, 1372),\n",
              " (55, 1365),\n",
              " (55, 1077),\n",
              " (55, 1170),\n",
              " (1077, 1365),\n",
              " (1077, 55),\n",
              " (1077, 1170),\n",
              " (1077, 1140),\n",
              " (1170, 55),\n",
              " (1170, 1077),\n",
              " (1170, 1140),\n",
              " (1170, 797),\n",
              " (1140, 1077),\n",
              " (1140, 1170),\n",
              " (1140, 797),\n",
              " (1140, 863),\n",
              " (797, 1170),\n",
              " (797, 1140),\n",
              " (797, 863),\n",
              " (797, 550),\n",
              " (863, 1140),\n",
              " (863, 797),\n",
              " (863, 550),\n",
              " (863, 1303),\n",
              " (550, 797),\n",
              " (550, 863),\n",
              " (550, 1303),\n",
              " (1303, 863),\n",
              " (1303, 550),\n",
              " (1303, 887),\n",
              " (887, 1303),\n",
              " (887, 452),\n",
              " (452, 887),\n",
              " (870, 1266),\n",
              " (1266, 870),\n",
              " (1266, 312),\n",
              " (1266, 506),\n",
              " (312, 1266),\n",
              " (312, 506),\n",
              " (312, 0),\n",
              " (506, 1266),\n",
              " (506, 312),\n",
              " (506, 0),\n",
              " (506, 1126),\n",
              " (0, 312),\n",
              " (0, 506),\n",
              " (0, 1126),\n",
              " (0, 1152),\n",
              " (1126, 506),\n",
              " (1126, 0),\n",
              " (1126, 1152),\n",
              " (1126, 153),\n",
              " (1152, 0),\n",
              " (1152, 1126),\n",
              " (1152, 153),\n",
              " (1152, 304),\n",
              " (153, 1126),\n",
              " (153, 1152),\n",
              " (153, 304),\n",
              " (153, 33),\n",
              " (304, 1152),\n",
              " (304, 153),\n",
              " (304, 33),\n",
              " (304, 612),\n",
              " (33, 153),\n",
              " (33, 304),\n",
              " (33, 612),\n",
              " (33, 1267),\n",
              " (612, 304),\n",
              " (612, 33),\n",
              " (612, 1267),\n",
              " (1267, 33),\n",
              " (1267, 612),\n",
              " (1267, 1271),\n",
              " (1271, 1267),\n",
              " (1271, 1398),\n",
              " (1271, 837),\n",
              " (1398, 1271),\n",
              " (1398, 837),\n",
              " (837, 1271),\n",
              " (837, 1398),\n",
              " (837, 123),\n",
              " (123, 837),\n",
              " (123, 44),\n",
              " (44, 123),\n",
              " (1266, 312),\n",
              " (1266, 506),\n",
              " (312, 1266),\n",
              " (312, 506),\n",
              " (312, 343),\n",
              " (506, 1266),\n",
              " (506, 312),\n",
              " (506, 343),\n",
              " (506, 1274),\n",
              " (343, 312),\n",
              " (343, 506),\n",
              " (343, 1274),\n",
              " (343, 1392),\n",
              " (1274, 506),\n",
              " (1274, 343),\n",
              " (1274, 1392),\n",
              " (1274, 147),\n",
              " (1392, 343),\n",
              " (1392, 1274),\n",
              " (1392, 147),\n",
              " (1392, 1274),\n",
              " (147, 1274),\n",
              " (147, 1392),\n",
              " (147, 1274),\n",
              " (147, 28),\n",
              " (1274, 1392),\n",
              " (1274, 147),\n",
              " (1274, 28),\n",
              " (1274, 506),\n",
              " (28, 147),\n",
              " (28, 1274),\n",
              " (28, 506),\n",
              " (28, 0),\n",
              " (506, 1274),\n",
              " (506, 28),\n",
              " (506, 0),\n",
              " (506, 1057),\n",
              " (0, 28),\n",
              " (0, 506),\n",
              " (0, 1057),\n",
              " (0, 33),\n",
              " (1057, 506),\n",
              " (1057, 0),\n",
              " (1057, 33),\n",
              " (1057, 1267),\n",
              " (33, 0),\n",
              " (33, 1057),\n",
              " (33, 1267),\n",
              " (1267, 1057),\n",
              " (1267, 33),\n",
              " (1267, 378),\n",
              " (378, 1267),\n",
              " (378, 1398),\n",
              " (378, 1264),\n",
              " (1398, 378),\n",
              " (1398, 1264),\n",
              " (1264, 378),\n",
              " (1264, 1398),\n",
              " (1264, 1478),\n",
              " (1478, 1264),\n",
              " (1478, 624),\n",
              " (624, 1478),\n",
              " (1274, 267),\n",
              " (1274, 1283),\n",
              " (267, 1274),\n",
              " (267, 1283),\n",
              " (267, 1274),\n",
              " (1283, 1274),\n",
              " (1283, 267),\n",
              " (1283, 1274),\n",
              " (1283, 225),\n",
              " (1274, 267),\n",
              " (1274, 1283),\n",
              " (1274, 225),\n",
              " (225, 1283),\n",
              " (225, 1274),\n",
              " (225, 624),\n",
              " (624, 225),\n",
              " (624, 606),\n",
              " (624, 40),\n",
              " (606, 624),\n",
              " (606, 40),\n",
              " (606, 436),\n",
              " (40, 624),\n",
              " (40, 606),\n",
              " (40, 436),\n",
              " (40, 884),\n",
              " (436, 606),\n",
              " (436, 40),\n",
              " (436, 884),\n",
              " (436, 3),\n",
              " (884, 40),\n",
              " (884, 436),\n",
              " (884, 3),\n",
              " (884, 1266),\n",
              " (3, 436),\n",
              " (3, 884),\n",
              " (3, 1266),\n",
              " (3, 954),\n",
              " (1266, 884),\n",
              " (1266, 3),\n",
              " (1266, 954),\n",
              " (954, 3),\n",
              " (954, 1266),\n",
              " (796, 953),\n",
              " (796, 1398),\n",
              " (953, 796),\n",
              " (953, 1398),\n",
              " (953, 796),\n",
              " (1398, 796),\n",
              " (1398, 953),\n",
              " (1398, 796),\n",
              " (1398, 312),\n",
              " (796, 953),\n",
              " (796, 1398),\n",
              " (796, 312),\n",
              " (796, 1112),\n",
              " (312, 1398),\n",
              " (312, 796),\n",
              " (312, 1112),\n",
              " (312, 147),\n",
              " (1112, 796),\n",
              " (1112, 312),\n",
              " (1112, 147),\n",
              " (1112, 1274),\n",
              " (147, 312),\n",
              " (147, 1112),\n",
              " (147, 1274),\n",
              " (147, 507),\n",
              " (1274, 1112),\n",
              " (1274, 147),\n",
              " (1274, 507),\n",
              " (1274, 767),\n",
              " (507, 147),\n",
              " (507, 1274),\n",
              " (507, 767),\n",
              " (507, 428),\n",
              " (767, 1274),\n",
              " (767, 507),\n",
              " (767, 428),\n",
              " (767, 1070),\n",
              " (428, 507),\n",
              " (428, 767),\n",
              " (428, 1070),\n",
              " (428, 1486),\n",
              " (1070, 767),\n",
              " (1070, 428),\n",
              " (1070, 1486),\n",
              " (1070, 612),\n",
              " (1486, 428),\n",
              " (1486, 1070),\n",
              " (1486, 612),\n",
              " (1486, 363),\n",
              " (612, 1070),\n",
              " (612, 1486),\n",
              " (612, 363),\n",
              " (612, 796),\n",
              " (363, 1486),\n",
              " (363, 612),\n",
              " (363, 796),\n",
              " (363, 311),\n",
              " (796, 612),\n",
              " (796, 363),\n",
              " (796, 311),\n",
              " (311, 363),\n",
              " (311, 796),\n",
              " (311, 1077),\n",
              " (1077, 311),\n",
              " (1077, 267),\n",
              " (1077, 534),\n",
              " (267, 1077),\n",
              " (267, 534),\n",
              " (267, 0),\n",
              " (534, 1077),\n",
              " (534, 267),\n",
              " (534, 0),\n",
              " (534, 1112),\n",
              " (0, 267),\n",
              " (0, 534),\n",
              " (0, 1112),\n",
              " (0, 79),\n",
              " (1112, 534),\n",
              " (1112, 0),\n",
              " (1112, 79),\n",
              " (1112, 550),\n",
              " (79, 0),\n",
              " (79, 1112),\n",
              " (79, 550),\n",
              " (79, 1112),\n",
              " (550, 1112),\n",
              " (550, 79),\n",
              " (550, 1112),\n",
              " (550, 33),\n",
              " (1112, 79),\n",
              " (1112, 550),\n",
              " (1112, 33),\n",
              " (1112, 550),\n",
              " (33, 550),\n",
              " (33, 1112),\n",
              " (33, 550),\n",
              " (550, 1112),\n",
              " (550, 33),\n",
              " (1418, 55),\n",
              " (55, 1418),\n",
              " (55, 55),\n",
              " (55, 55),\n",
              " (55, 624),\n",
              " (55, 1398),\n",
              " (624, 55),\n",
              " (624, 1398),\n",
              " (624, 949),\n",
              " (1398, 55),\n",
              " (1398, 624),\n",
              " (1398, 949),\n",
              " (1398, 1307),\n",
              " (949, 624),\n",
              " (949, 1398),\n",
              " (949, 1307),\n",
              " (949, 75),\n",
              " (1307, 1398),\n",
              " (1307, 949),\n",
              " (1307, 75),\n",
              " (75, 949),\n",
              " (75, 1307),\n",
              " (1266, 312),\n",
              " (312, 1266),\n",
              " (312, 1307),\n",
              " (1307, 312),\n",
              " (1307, 1283),\n",
              " (1307, 1421),\n",
              " (1283, 1307),\n",
              " (1283, 1421),\n",
              " (1283, 1266),\n",
              " (1421, 1307),\n",
              " (1421, 1283),\n",
              " (1421, 1266),\n",
              " (1266, 1283),\n",
              " (1266, 1421),\n",
              " (1266, 1478),\n",
              " (1478, 1266),\n",
              " (1478, 1042),\n",
              " (1478, 606),\n",
              " (1042, 1478),\n",
              " (1042, 606),\n",
              " (1042, 1266),\n",
              " (606, 1478),\n",
              " (606, 1042),\n",
              " (606, 1266),\n",
              " (606, 954),\n",
              " (1266, 1042),\n",
              " (1266, 606),\n",
              " (1266, 954),\n",
              " (1266, 53),\n",
              " (954, 606),\n",
              " (954, 1266),\n",
              " (954, 53),\n",
              " (954, 612),\n",
              " (53, 1266),\n",
              " (53, 954),\n",
              " (53, 612),\n",
              " (53, 1266),\n",
              " (612, 954),\n",
              " (612, 53),\n",
              " (612, 1266),\n",
              " (612, 1208),\n",
              " (1266, 53),\n",
              " (1266, 612),\n",
              " (1266, 1208),\n",
              " (1208, 612),\n",
              " (1208, 1266),\n",
              " (1266, 312),\n",
              " (1266, 652),\n",
              " (312, 1266),\n",
              " (312, 652),\n",
              " (312, 1264),\n",
              " (652, 1266),\n",
              " (652, 312),\n",
              " (652, 1264),\n",
              " (652, 1266),\n",
              " (1264, 312),\n",
              " (1264, 652),\n",
              " (1264, 1266),\n",
              " (1264, 954),\n",
              " (1266, 652),\n",
              " (1266, 1264),\n",
              " (1266, 954),\n",
              " (1266, 506),\n",
              " (954, 1264),\n",
              " (954, 1266),\n",
              " (954, 506),\n",
              " (954, 0),\n",
              " (506, 1266),\n",
              " (506, 954),\n",
              " (506, 0),\n",
              " (506, 1126),\n",
              " (0, 954),\n",
              " (0, 506),\n",
              " (0, 1126),\n",
              " (0, 1152),\n",
              " (1126, 506),\n",
              " (1126, 0),\n",
              " (1126, 1152),\n",
              " (1126, 1315),\n",
              " (1152, 0),\n",
              " (1152, 1126),\n",
              " (1152, 1315),\n",
              " (1152, 147),\n",
              " (1315, 1126),\n",
              " (1315, 1152),\n",
              " (1315, 147),\n",
              " (1315, 1274),\n",
              " (147, 1152),\n",
              " (147, 1315),\n",
              " (147, 1274),\n",
              " (147, 506),\n",
              " (1274, 1315),\n",
              " (1274, 147),\n",
              " (1274, 506),\n",
              " (1274, 825),\n",
              " (506, 147),\n",
              " (506, 1274),\n",
              " (506, 825),\n",
              " (506, 337),\n",
              " (825, 1274),\n",
              " (825, 506),\n",
              " (825, 337),\n",
              " (825, 1064),\n",
              " (337, 506),\n",
              " (337, 825),\n",
              " (337, 1064),\n",
              " (337, 561),\n",
              " (1064, 825),\n",
              " (1064, 337),\n",
              " (1064, 561),\n",
              " (561, 337),\n",
              " (561, 1064),\n",
              " (1288, 123),\n",
              " (1288, 1398),\n",
              " (123, 1288),\n",
              " (123, 1398),\n",
              " (123, 36),\n",
              " (1398, 1288),\n",
              " (1398, 123),\n",
              " (1398, 36),\n",
              " (1398, 481),\n",
              " (36, 123),\n",
              " (36, 1398),\n",
              " (36, 481),\n",
              " (36, 999),\n",
              " (481, 1398),\n",
              " (481, 36),\n",
              " (481, 999),\n",
              " (481, 428),\n",
              " (999, 36),\n",
              " (999, 481),\n",
              " (999, 428),\n",
              " (999, 641),\n",
              " (428, 481),\n",
              " (428, 999),\n",
              " (428, 641),\n",
              " (428, 1266),\n",
              " (641, 999),\n",
              " (641, 428),\n",
              " (641, 1266),\n",
              " (641, 954),\n",
              " (1266, 428),\n",
              " (1266, 641),\n",
              " (1266, 954),\n",
              " (1266, 64),\n",
              " (954, 641),\n",
              " (954, 1266),\n",
              " (954, 64),\n",
              " (954, 1274),\n",
              " (64, 1266),\n",
              " (64, 954),\n",
              " (64, 1274),\n",
              " (64, 267),\n",
              " (1274, 954),\n",
              " (1274, 64),\n",
              " (1274, 267),\n",
              " (1274, 1391),\n",
              " (267, 64),\n",
              " (267, 1274),\n",
              " (267, 1391),\n",
              " (267, 304),\n",
              " (1391, 1274),\n",
              " (1391, 267),\n",
              " (1391, 304),\n",
              " (304, 267),\n",
              " (304, 1391),\n",
              " (304, 1451),\n",
              " (1451, 304),\n",
              " (1451, 0),\n",
              " (0, 1451),\n",
              " (0, 704),\n",
              " (704, 0),\n",
              " (704, 1264),\n",
              " (1264, 704),\n",
              " (1424, 795),\n",
              " (1424, 33),\n",
              " (795, 1424),\n",
              " (795, 33),\n",
              " (795, 796),\n",
              " (33, 1424),\n",
              " (33, 795),\n",
              " (33, 796),\n",
              " (33, 311),\n",
              " (796, 795),\n",
              " (796, 33),\n",
              " (796, 311),\n",
              " (796, 1457),\n",
              " (311, 33),\n",
              " (311, 796),\n",
              " (311, 1457),\n",
              " (311, 1360),\n",
              " (1457, 796),\n",
              " (1457, 311),\n",
              " (1457, 1360),\n",
              " (1457, 870),\n",
              " (1360, 311),\n",
              " (1360, 1457),\n",
              " (1360, 870),\n",
              " (1360, 1266),\n",
              " (870, 1457),\n",
              " (870, 1360),\n",
              " (870, 1266),\n",
              " (1266, 1360),\n",
              " (1266, 870),\n",
              " (1266, 491),\n",
              " (491, 1266),\n",
              " (491, 883),\n",
              " (883, 491),\n",
              " (883, 1204),\n",
              " (1204, 883),\n",
              " (1204, 1271),\n",
              " (1271, 1204),\n",
              " (1271, 1398),\n",
              " (1271, 853),\n",
              " (1398, 1271),\n",
              " (1398, 853),\n",
              " (1398, 3),\n",
              " (853, 1271),\n",
              " (853, 1398),\n",
              " (853, 3),\n",
              " (853, 1266),\n",
              " (3, 1398),\n",
              " (3, 853),\n",
              " (3, 1266),\n",
              " (1266, 853),\n",
              " (1266, 3),\n",
              " (1266, 1117),\n",
              " (1117, 1266),\n",
              " (1117, 885),\n",
              " (1117, 1307),\n",
              " (885, 1117),\n",
              " (885, 1307),\n",
              " (1307, 1117),\n",
              " (1307, 885),\n",
              " (1307, 1264),\n",
              " (1264, 1307),\n",
              " (1264, 1206),\n",
              " (1264, 33),\n",
              " (1206, 1264),\n",
              " (1206, 33),\n",
              " (1206, 808),\n",
              " (33, 1264),\n",
              " (33, 1206),\n",
              " (33, 808),\n",
              " (33, 1282),\n",
              " (808, 1206),\n",
              " (808, 33),\n",
              " (808, 1282),\n",
              " (808, 1478),\n",
              " (1282, 33),\n",
              " (1282, 808),\n",
              " (1282, 1478),\n",
              " (1282, 1153),\n",
              " (1478, 808),\n",
              " (1478, 1282),\n",
              " (1478, 1153),\n",
              " (1478, 75),\n",
              " (1153, 1282),\n",
              " (1153, 1478),\n",
              " (1153, 75),\n",
              " (75, 1478),\n",
              " (75, 1153),\n",
              " (75, 21),\n",
              " (21, 75),\n",
              " (21, 887),\n",
              " (21, 1266),\n",
              " (887, 21),\n",
              " (887, 1266),\n",
              " (1266, 21),\n",
              " (1266, 887),\n",
              " (795, 311),\n",
              " (311, 795),\n",
              " (311, 55),\n",
              " (55, 311),\n",
              " (55, 537),\n",
              " (55, 919),\n",
              " (537, 55),\n",
              " (537, 919),\n",
              " (537, 884),\n",
              " (919, 55),\n",
              " (919, 537),\n",
              " (919, 884),\n",
              " (919, 563),\n",
              " (884, 537),\n",
              " (884, 919),\n",
              " (884, 563),\n",
              " (884, 786),\n",
              " (563, 919),\n",
              " (563, 884),\n",
              " (563, 786),\n",
              " (786, 884),\n",
              " (786, 563),\n",
              " (428, 1470),\n",
              " (428, 33),\n",
              " (1470, 428),\n",
              " (1470, 33),\n",
              " (1470, 796),\n",
              " (33, 428),\n",
              " (33, 1470),\n",
              " (33, 796),\n",
              " (33, 311),\n",
              " (796, 1470),\n",
              " (796, 33),\n",
              " (796, 311),\n",
              " (311, 33),\n",
              " (311, 796),\n",
              " (311, 64),\n",
              " (64, 311),\n",
              " (64, 55),\n",
              " (55, 64),\n",
              " (55, 1077),\n",
              " (1077, 55),\n",
              " (1077, 0),\n",
              " (0, 1077),\n",
              " (0, 304),\n",
              " (304, 0),\n",
              " (304, 619),\n",
              " (304, 563),\n",
              " (619, 304),\n",
              " (619, 563),\n",
              " (619, 559),\n",
              " (563, 304),\n",
              " (563, 619),\n",
              " (563, 559),\n",
              " (563, 176),\n",
              " (559, 619),\n",
              " (559, 563),\n",
              " (559, 176),\n",
              " (176, 563),\n",
              " (176, 559),\n",
              " (841, 863),\n",
              " (841, 1268),\n",
              " (863, 841),\n",
              " (863, 1268),\n",
              " (863, 855),\n",
              " (1268, 841),\n",
              " (1268, 863),\n",
              " (1268, 855),\n",
              " (1268, 0),\n",
              " (855, 863),\n",
              " (855, 1268),\n",
              " (855, 0),\n",
              " (855, 666),\n",
              " (0, 1268),\n",
              " (0, 855),\n",
              " (0, 666),\n",
              " (666, 855),\n",
              " (666, 0),\n",
              " (666, 889),\n",
              " (889, 666),\n",
              " (889, 907),\n",
              " (907, 889),\n",
              " ...]"
            ]
          },
          "metadata": {},
          "execution_count": 32
        }
      ],
      "source": [
        "# Now we can train the word2vec\n",
        "\n",
        "# Let's think about training pairs\n",
        "index_pairs # this is our dataset. It's list of list of two integer\n",
        "# two integer means a pair of neighboring words\n",
        "\n",
        "# Training set and Test set\n",
        "# To validate that our model can solve 'unseen' problems\n",
        "# So we have to split the dataset before training.\n",
        "\n",
        "# To randomly split the dataset, we will first shuffle the dataset\n",
        "\n",
        "# random.shuffle(index_pairs) # this will shuffle the list items"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "TC0QF3NZp9Rh",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 141
        },
        "outputId": "f4cf8d6c-f483-443b-a2a1-d34847df1fd1"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'train_set' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-33-0776c2c2e112>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_set\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_set\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'train_set' is not defined"
          ]
        }
      ],
      "source": [
        "len(train_set), len(test_set)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CKMeat2TyhDZ"
      },
      "source": [
        "## 16. Training the Word2Vec Model\n",
        "\n",
        "Now we'll train our Word2Vec model using batched gradient descent:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JpUM3aiiqG8e"
      },
      "outputs": [],
      "source": [
        "# making batch from train_set\n",
        "# Batch is a set of training samples, that are calculated together\n",
        "# And also we update the model after one single batch"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s2frfncyyhDZ"
      },
      "source": [
        "## 17. Evaluating the Training\n",
        "\n",
        "Let's visualize the training loss to see if our model is learning:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FJHSV8zbyLYu"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "plt.plot(loss_record)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7tZxnhPdyhDZ"
      },
      "source": [
        "## 18. Testing the Model\n",
        "\n",
        "Now we'll test our model on the test set:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RDP9aR48zdJu"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2PPgR_2dyhDZ"
      },
      "source": [
        "## 19. Exploring Learned Word Relationships\n",
        "\n",
        "Let's explore what our model has learned by finding the words most closely related to \"harry\":"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0YaCDNq_0hFc"
      },
      "outputs": [],
      "source": [
        "# P(potter|harry)?\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}